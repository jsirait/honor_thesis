{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary look at subreddits for 6k news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zstandard\n",
      "  Downloading zstandard-0.16.0-cp39-cp39-win_amd64.whl (733 kB)\n",
      "Installing collected packages: zstandard\n",
      "Successfully installed zstandard-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import io\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import datetime, time\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 19:31:03.435559\n",
      "19:31:03\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print(str(datetime.datetime.now())[11:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gm_intersection.json\", \"r\") as infile:\n",
    "    news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40645"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['websterprogresstimes.com',\n",
       " 'cordeledispatch.com',\n",
       " 'ukconstructionmedia.co.uk',\n",
       " 'dylanpaulus.com',\n",
       " 'arktimes.com',\n",
       " 'asiafoodjournal.com',\n",
       " 'corydontimes.com',\n",
       " 'stuttgartdailyleader.com',\n",
       " 'artrockermagazine.com',\n",
       " 'caixinglobal.com']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open reddit data from April 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://arxiv.org/pdf/2001.08435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an entry of data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'all_awardings': [], \n",
    "    'allow_live_comments': False, \n",
    "    'archived': False, \n",
    "    'author': 'elanglohablante9805', \n",
    "    'author_created_utc': 1609519842, \n",
    "    'author_flair_background_color': '#ffb000', \n",
    "    'author_flair_css_class': None, \n",
    "    'author_flair_richtext': [], \n",
    "    'author_flair_template_id': '4f908eaa-9664-11ea-a567-0ed46a42aec3', \n",
    "    'author_flair_text': 'Historiador 游닆 | 80-Day Streak 游댠', \n",
    "    'author_flair_text_color': 'dark', \n",
    "    'author_flair_type': 'text', \n",
    "    'author_fullname': 't2_9lr431i4', \n",
    "    'author_patreon_flair': False, \n",
    "    'author_premium': False, \n",
    "    'can_gild': True, \n",
    "    'category': None, \n",
    "    'content_categories': None, \n",
    "    'contest_mode': False, \n",
    "    'created_utc': 1617235201, \n",
    "    'discussion_type': None, \n",
    "    'distinguished': None, \n",
    "    'domain': 'self.WriteStreakES', \n",
    "    'edited': False, \n",
    "    'gilded': 0, \n",
    "    'gildings': {}, \n",
    "    'hidden': False, \n",
    "    'hide_score': False, \n",
    "    'id': 'mhj2hj', \n",
    "    'is_created_from_ads_ui': False, \n",
    "    'is_crosspostable': True, \n",
    "    'is_meta': False, \n",
    "    'is_original_content': False, \n",
    "    'is_reddit_media_domain': False, \n",
    "    'is_robot_indexable': True, \n",
    "    'is_self': True, \n",
    "    'is_video': False, \n",
    "    'link_flair_background_color': '', \n",
    "    'link_flair_css_class': None, \n",
    "    'link_flair_richtext': [], \n",
    "    'link_flair_text': None, \n",
    "    'link_flair_text_color': 'dark', \n",
    "    'link_flair_type': 'text', \n",
    "    'locked': False,\n",
    "    'media': None, \n",
    "    'media_embed': {}, \n",
    "    'media_only': False, \n",
    "    'name': 't3_mhj2hj', \n",
    "    'no_follow': True, \n",
    "    'num_comments': 2, \n",
    "    'num_crossposts': 0, \n",
    "    'over_18': False, \n",
    "    'parent_whitelist_status': None, \n",
    "    'permalink': '/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'pinned': False, \n",
    "    'pwls': None, \n",
    "    'quarantine': False, \n",
    "    'removed_by_category': None, \n",
    "    'retrieved_utc': 1623447663, \n",
    "    'score': 1, \n",
    "    'secure_media': None, \n",
    "    'secure_media_embed': {}, \n",
    "    'selftext': 'Los p치jaros est치n cantando, las hierbas verdes est치n brotando, y tengo alergias.  Esto es la temporada de las alergias.  Estornudo cada ma침ana cuando me despierto, y otra vez si voy afuera.  Necesito tomar medicina cada d칤a, pero no funciona tan bien. \\n\\nPor fuera, las lomas son bonitas porque son verdes y los robles tienen hojas nuevas.  Por el fin de semana,  hago caminatas pero cuando regreso a casa, necesito ducharme para remover el polen.\\n\\nCuando me jubile, voy a viajar al desierto cada a침o por toda la primavera.  No me gustar칤a quedarme aqu칤.', \n",
    "    'send_replies': True, \n",
    "    'spoiler': False, \n",
    "    'stickied': False, \n",
    "    'subreddit': 'WriteStreakES', \n",
    "    'subreddit_id': 't5_2eamt5', \n",
    "    'subreddit_subscribers': 2205, \n",
    "    'subreddit_type': 'public', \n",
    "    'suggested_sort': None, \n",
    "    'thumbnail': 'self', \n",
    "    'thumbnail_height': None, \n",
    "    'thumbnail_width': None, \n",
    "    'title': 'Streak 90: Ha llegado la primavera', \n",
    "    'top_awarded_type': None, \n",
    "    'total_awards_received': 0, \n",
    "    'treatment_tags': [], \n",
    "    'upvote_ratio': 1.0, \n",
    "    'url': 'https://www.reddit.com/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'whitelist_status': None, 'wls': None}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctx = zstd.ZstdDecompressor(max_window_size=2147483648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findURLs(phrase):\n",
    "    regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)')\n",
    "    url = re.findall(regex, phrase)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lol']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out\n",
    "findURLs(\"does this find https://lol or nytimes.com/2021/10/19/us/politics/trump-border.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname(url, uri_type='both'):\n",
    "    \"\"\"Get the host name from the url\"\"\"\n",
    "    # domain = re.compile(r\"(https?://)?(www\\.)?\")\n",
    "    # return domain.sub('', url).strip().strip('/').split('/')[0]\n",
    "    extracted = tldextract.extract(url)\n",
    "    subdomain, domain, suffix = extracted\n",
    "    # add both versions of domain.suffix and subdomain.domain.suffix\n",
    "    full = \"\"\n",
    "    if len(subdomain) > 0:\n",
    "        #print(f\"{subdomain}.{domain}.{suffix}\")\n",
    "        full = f\"{subdomain}.{domain}.{suffix}\"\n",
    "    else:\n",
    "        full = f\"{domain}.{suffix}\"\n",
    "    if len(full) > 0:\n",
    "        full = full.replace(\"www.\",\"\").strip(\"/\")\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes.com\n",
      "aiaia.nytimes.com\n",
      "nytimes.com\n"
     ]
    }
   ],
   "source": [
    "# function try out\n",
    "print(get_hostname(\"https://www.nytimes.com\"))\n",
    "print(get_hostname(\"http://www.aiaia.nytimes.com/add\"))\n",
    "print(get_hostname(\"www.nytimes.com/additional\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"realtor.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zst_files = [\"RS_2021-01.zst\", \"RS_2021-02.zst\", \"RS_2021-03.zst\", \"RS_2021-04.zst\", \"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "# zst_files = [\"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "zst_filepath = \"E:/thesis_data/\" # D for ThinkPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "subreddit_ns = defaultdict(Counter)\n",
    "subreddit_srid = dict()\n",
    "\n",
    "posts_with_urls = list()\n",
    "posts_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2021-11-27 05:19:54.611990\n",
      "***** Start processing for RS_2021-05.zst *****\n",
      "processed 500000 by 05:21:56\n",
      "processed 1000000 by 05:23:53\n",
      "processed 1500000 by 05:25:44\n",
      "processed 2000000 by 05:27:37\n",
      "processed 2500000 by 05:29:28\n",
      "processed 3000000 by 05:31:22\n",
      "processed 3500000 by 05:33:12\n",
      "processed 4000000 by 05:35:06\n",
      "processed 4500000 by 05:37:05\n",
      "processed 5000000 by 05:39:00\n",
      "processed 5500000 by 05:40:55\n",
      "processed 6000000 by 05:42:45\n",
      "processed 6500000 by 05:44:39\n",
      "processed 7000000 by 05:46:33\n",
      "processed 7500000 by 05:48:25\n",
      "processed 8000000 by 05:50:21\n",
      "processed 8500000 by 05:52:12\n",
      "processed 9000000 by 05:54:05\n",
      "processed 9500000 by 05:55:55\n",
      "processed 10000000 by 05:57:47\n",
      "processed 10500000 by 05:59:54\n",
      "processed 11000000 by 06:01:55\n",
      "processed 11500000 by 06:03:51\n",
      "processed 12000000 by 06:05:49\n",
      "processed 12500000 by 06:07:49\n",
      "processed 13000000 by 06:10:25\n",
      "processed 13500000 by 06:12:44\n",
      "processed 14000000 by 06:14:45\n",
      "processed 14500000 by 06:16:45\n",
      "processed 15000000 by 06:18:47\n",
      "processed 15500000 by 06:20:45\n",
      "processed 16000000 by 06:24:36\n",
      "processed 16500000 by 06:27:33\n",
      "processed 17000000 by 06:29:39\n",
      "processed 17500000 by 06:31:40\n",
      "processed 18000000 by 06:33:41\n",
      "processed 18500000 by 06:35:42\n",
      "processed 19000000 by 06:37:37\n",
      "processed 19500000 by 06:39:39\n",
      "processed 20000000 by 06:41:36\n",
      "processed 20500000 by 06:47:10\n",
      "processed 21000000 by 06:50:01\n",
      "processed 21500000 by 06:51:55\n",
      "processed 22000000 by 06:53:54\n",
      "processed 22500000 by 06:55:52\n",
      "processed 23000000 by 06:57:49\n",
      "processed 23500000 by 06:59:52\n",
      "processed 24000000 by 07:01:50\n",
      "processed 24500000 by 07:03:51\n",
      "processed 25000000 by 07:05:52\n",
      "processed 25500000 by 07:15:19\n",
      "processed 26000000 by 07:18:15\n",
      "processed 26500000 by 07:20:14\n",
      "processed 27000000 by 07:22:18\n",
      "processed 27500000 by 07:24:18\n",
      "processed 28000000 by 07:26:22\n",
      "processed 28500000 by 07:28:22\n",
      "processed 29000000 by 07:30:23\n",
      "processed 29500000 by 07:32:24\n",
      "processed 30000000 by 07:34:24\n",
      "processed 30500000 by 07:36:27\n",
      "processed 31000000 by 07:38:31\n",
      "processed 31500000 by 07:40:39\n",
      "processed 32000000 by 07:42:44\n",
      "processed 32500000 by 07:44:49\n",
      "processed 33000000 by 07:46:54\n",
      "processed 33500000 by 08:01:14\n",
      "processed 34000000 by 08:04:41\n",
      "processed 34500000 by 08:06:53\n",
      "processed 35000000 by 08:09:04\n",
      "processed 35500000 by 08:11:10\n",
      "processed 36000000 by 08:13:18\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-05.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-06.zst *****\n",
      "processed 500000 by 08:31:27\n",
      "processed 1000000 by 08:33:47\n",
      "processed 1500000 by 08:35:49\n",
      "processed 2000000 by 08:37:55\n",
      "processed 2500000 by 08:39:58\n",
      "processed 3000000 by 08:42:00\n",
      "processed 3500000 by 08:44:06\n",
      "processed 4000000 by 08:46:06\n",
      "processed 4500000 by 08:48:13\n",
      "processed 5000000 by 08:50:16\n",
      "processed 5500000 by 08:52:20\n",
      "processed 6000000 by 08:54:25\n",
      "processed 6500000 by 08:56:23\n",
      "processed 7000000 by 08:58:26\n",
      "processed 7500000 by 09:00:32\n",
      "processed 8000000 by 09:02:37\n",
      "processed 8500000 by 09:04:41\n",
      "processed 9000000 by 09:31:41\n",
      "processed 9500000 by 09:34:17\n",
      "processed 10000000 by 09:36:22\n",
      "processed 10500000 by 09:38:29\n",
      "processed 11000000 by 09:40:33\n",
      "processed 11500000 by 09:42:37\n",
      "processed 12000000 by 09:44:43\n",
      "processed 12500000 by 09:46:45\n",
      "processed 13000000 by 09:48:52\n",
      "processed 13500000 by 09:50:56\n",
      "processed 14000000 by 09:53:01\n",
      "processed 14500000 by 09:55:04\n",
      "processed 15000000 by 09:57:03\n",
      "processed 15500000 by 09:59:04\n",
      "processed 16000000 by 10:01:06\n",
      "processed 16500000 by 10:03:11\n",
      "processed 17000000 by 10:05:15\n",
      "processed 17500000 by 10:07:20\n",
      "processed 18000000 by 10:09:22\n",
      "processed 18500000 by 10:11:25\n",
      "processed 19000000 by 10:13:28\n",
      "processed 19500000 by 10:15:28\n",
      "processed 20000000 by 10:17:33\n",
      "processed 20500000 by 10:19:33\n",
      "processed 21000000 by 10:21:34\n",
      "processed 21500000 by 10:23:34\n",
      "processed 22000000 by 11:03:09\n",
      "processed 22500000 by 11:05:50\n",
      "processed 23000000 by 11:07:53\n",
      "processed 23500000 by 11:10:00\n",
      "processed 24000000 by 11:12:04\n",
      "processed 24500000 by 11:14:07\n",
      "processed 25000000 by 11:16:07\n",
      "processed 25500000 by 11:18:11\n",
      "processed 26000000 by 11:20:11\n",
      "processed 26500000 by 11:22:15\n",
      "processed 27000000 by 11:24:17\n",
      "processed 27500000 by 11:26:19\n",
      "processed 28000000 by 11:28:21\n",
      "processed 28500000 by 11:30:23\n",
      "processed 29000000 by 11:32:27\n",
      "processed 29500000 by 11:34:28\n",
      "processed 30000000 by 11:36:30\n",
      "processed 30500000 by 11:38:34\n",
      "processed 31000000 by 11:40:38\n",
      "processed 31500000 by 11:42:42\n",
      "processed 32000000 by 11:44:45\n",
      "processed 32500000 by 11:46:48\n",
      "processed 33000000 by 11:48:50\n",
      "processed 33500000 by 11:50:53\n",
      "processed 34000000 by 11:52:57\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-06.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "finish time: 2021-11-27 12:12:19.900386\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "for zst_file in zst_files[:1]:\n",
    "    print(\"***** Start processing for {} *****\".format(zst_file))\n",
    "    with open(zst_filepath+zst_file, 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "        with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            url_of_our_ns = False \n",
    "            # ^this is to keep track if this post has ns url that we care about.\n",
    "            # if so, add to posts_with_urls once.\n",
    "            for d in text_stream:\n",
    "                line = json.loads(d)\n",
    "                subreddit, subreddit_id = line['subreddit'], line['subreddit_id']\n",
    "                if subreddit not in subreddit_srid:\n",
    "                    subreddit_srid[subreddit] = subreddit_id\n",
    "                URLs = findURLs(line['url']) + findURLs(line['selftext'])\n",
    "                URLs = [get_hostname(url) for url in URLs]\n",
    "                # print(\"URLs:\", URLs)\n",
    "                # if len(URLs) > 10: print(line['selftext'])\n",
    "                for url in URLs:\n",
    "                    if url in news_sources:\n",
    "                        if not url_of_our_ns:\n",
    "                            posts_with_urls.append(line)\n",
    "                            url_of_our_ns = True\n",
    "                        ns_subreddit[url][subreddit] += 1\n",
    "                        subreddit_ns[subreddit][url] += 1\n",
    "                url_of_our_ns = False\n",
    "                counter += 1\n",
    "                if counter%500000 == 0: \n",
    "                    print(\"processed {} by {}\".format(counter, str(datetime.datetime.now())[11:19]))\n",
    "                \n",
    "    \n",
    "    print(\"-------------------------------- Done reading, will write files now --------------------------------\")\n",
    "    \n",
    "    # write into files separated by months\n",
    "    with open(\"ns_subreddit_{}.json\".format(zst_file[3:10]), \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(ns_subreddit, outfile)\n",
    "        \n",
    "    with open(\"subreddit_ns_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile1:\n",
    "        json.dump(subreddit_ns, outfile1)\n",
    "        \n",
    "    with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "        json.dump(subreddit_srid, infile_srid)\n",
    "        \n",
    "    with open(\"D:/thesis_data/posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "        json.dump(posts_with_urls, outfile2)\n",
    "        \n",
    "    ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    subreddit_ns = defaultdict(Counter)\n",
    "    # subreddit_srid = dict()\n",
    "    posts_with_urls = list()\n",
    "    counter = 0\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------- Done processing for {} --------------------------------\".format(zst_file))\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "#     json.dump(subreddit_srid, infile_srid)\n",
    "        \n",
    "# with open(\"D:/thesis_data/posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "#     json.dump(posts_with_urls, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-03'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zst_file[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of posts read from \n",
    "1. January 2021:\n",
    "2. February 2021: 31,161,912\n",
    "3. March 2021: 33,0061,03\n",
    "4. April 2021:\n",
    "5. May 2021: >36M\n",
    "6. June 2021: >34M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"subreddit_srid_{}.json\".format(\"2021-04\"), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "    json.dump(subreddit_srid, infile_srid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588719"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_with_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3686"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ns_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3686 news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddit_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20755 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811504"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_with_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ns_subreddit.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(ns_subreddit, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subreddit_ns.json\", \"w\", encoding = \"utf-8\") as outfile1:\n",
    "    json.dump(subreddit_ns, outfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"posts_with_urls\", \"w\", encoding = \"utf-8\") as outfile2:\n",
    "    json.dump(posts_with_urls, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31616206"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"counter_april21.json\", \"w\", encoding = \"utf-8\") as counterfile:\n",
    "    json.dump(counter, counterfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting subreddit names and ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_id = defaultdict(set)\n",
    "id_subreddit = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2021-11-05 01:59:23.609969\n",
      "at 500000\n",
      "at 1000000\n",
      "at 1500000\n",
      "at 2000000\n",
      "at 2500000\n",
      "at 3000000\n",
      "at 3500000\n",
      "at 4000000\n",
      "at 4500000\n",
      "at 5000000\n",
      "at 5500000\n",
      "at 6000000\n",
      "at 6500000\n",
      "at 7000000\n",
      "at 7500000\n",
      "at 8000000\n",
      "at 8500000\n",
      "at 9000000\n",
      "at 9500000\n",
      "at 10000000\n",
      "at 10500000\n",
      "at 11000000\n",
      "at 11500000\n",
      "at 12000000\n",
      "at 12500000\n",
      "at 13000000\n",
      "at 13500000\n",
      "at 14000000\n",
      "at 14500000\n",
      "at 15000000\n",
      "at 15500000\n",
      "at 16000000\n",
      "at 16500000\n",
      "at 17000000\n",
      "at 17500000\n",
      "at 18000000\n",
      "at 18500000\n",
      "at 19000000\n",
      "at 19500000\n",
      "at 20000000\n",
      "at 20500000\n",
      "at 21000000\n",
      "at 21500000\n",
      "at 22000000\n",
      "at 22500000\n",
      "at 23000000\n",
      "at 23500000\n",
      "at 24000000\n",
      "at 24500000\n",
      "at 25000000\n",
      "at 25500000\n",
      "at 26000000\n",
      "at 26500000\n",
      "at 27000000\n",
      "at 27500000\n",
      "at 28000000\n",
      "at 28500000\n",
      "at 29000000\n",
      "at 29500000\n",
      "at 30000000\n",
      "at 30500000\n",
      "at 31000000\n",
      "at 31500000\n",
      "finish time: 2021-11-05 03:35:19.988106\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "with open(\"D://Wellesley/F21/thesis_zst_data/RS_2021-04.zst\", 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "    with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "        text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "        for d in text_stream:\n",
    "            line = json.loads(d)\n",
    "            sr, sr_id = line['subreddit'], line['subreddit_id']\n",
    "            subreddit_id[sr].add(sr_id)\n",
    "            id_subreddit[sr_id].add(sr)\n",
    "#             URLs = findURLs(line['url']) + findURLs(line['selftext'])\n",
    "#             URLs = [get_hostname(url) for url in URLs]\n",
    "#             # print(\"URLs:\", URLs)\n",
    "#             # if len(URLs) > 10: print(line['selftext'])\n",
    "#             for url in URLs:\n",
    "#                 if url in news_sources:\n",
    "#                     posts_with_urls.append(line)\n",
    "#                     ns_subreddit[url][subreddit] += 1\n",
    "#                     subreddit_ns[subreddit][url] += 1\n",
    "            counter += 1\n",
    "            if counter%500000 == 0: print(f\"at {counter}\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639811"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639811"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddit_id:\n",
    "    if len(subreddit_id[s]) != 1:\n",
    "        print(f\"{s} is invalid, length {len(subreddit_id[s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in id_subreddit:\n",
    "    if len(id_subreddit[s]) != 1:\n",
    "        print(f\"{s} is invalid, length {len(id_subreddit[s])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make all values to be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddit_id:\n",
    "    subreddit_id[s] = list(subreddit_id[s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in id_subreddit:\n",
    "    id_subreddit[i] = list(id_subreddit[i])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the same number of `id`s and `subreddit`s. Good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subreddit_id.json\", \"w\", encoding = \"utf-8\") as outfile_si:\n",
    "    json.dump(subreddit_id, outfile_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"id_subreddit.json\", \"w\", encoding = \"utf-8\") as outfile_is:\n",
    "    json.dump(id_subreddit, outfile_is)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary look at subreddits for 6k news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zstandard\n",
      "  Downloading zstandard-0.16.0-cp39-cp39-win_amd64.whl (733 kB)\n",
      "Installing collected packages: zstandard\n",
      "Successfully installed zstandard-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import io\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import datetime, time\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-10 00:02:29.862281\n",
      "00:02:29\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print(str(datetime.datetime.now())[11:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gm_intersection.json\", \"r\") as infile:\n",
    "    news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42477"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['websterprogresstimes.com',\n",
       " 'cordeledispatch.com',\n",
       " 'k12.wv.us',\n",
       " 'ukconstructionmedia.co.uk',\n",
       " 'dylanpaulus.com',\n",
       " 'arktimes.com',\n",
       " 'asiafoodjournal.com',\n",
       " 'corydontimes.com',\n",
       " 'stuttgartdailyleader.com',\n",
       " 'artrockermagazine.com']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `gmm` instead\n",
    "\n",
    "Since there are *A LOT* of news sources in `gm_intersect` which is the intersection of gdelt and muckrack, let's instead use `gmm_intersect` which is the intersection of gdelt, muckrack,and mbfc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gmm_intersection.json\", \"r\") as infile:\n",
    "    gmm_news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gmm_news_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open reddit data from April 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://arxiv.org/pdf/2001.08435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an entry of data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'all_awardings': [], \n",
    "    'allow_live_comments': False, \n",
    "    'archived': False, \n",
    "    'author': 'elanglohablante9805', \n",
    "    'author_created_utc': 1609519842, \n",
    "    'author_flair_background_color': '#ffb000', \n",
    "    'author_flair_css_class': None, \n",
    "    'author_flair_richtext': [], \n",
    "    'author_flair_template_id': '4f908eaa-9664-11ea-a567-0ed46a42aec3', \n",
    "    'author_flair_text': 'Historiador 游닆 | 80-Day Streak 游댠', \n",
    "    'author_flair_text_color': 'dark', \n",
    "    'author_flair_type': 'text', \n",
    "    'author_fullname': 't2_9lr431i4', \n",
    "    'author_patreon_flair': False, \n",
    "    'author_premium': False, \n",
    "    'can_gild': True, \n",
    "    'category': None, \n",
    "    'content_categories': None, \n",
    "    'contest_mode': False, \n",
    "    'created_utc': 1617235201, \n",
    "    'discussion_type': None, \n",
    "    'distinguished': None, \n",
    "    'domain': 'self.WriteStreakES', \n",
    "    'edited': False, \n",
    "    'gilded': 0, \n",
    "    'gildings': {}, \n",
    "    'hidden': False, \n",
    "    'hide_score': False, \n",
    "    'id': 'mhj2hj', \n",
    "    'is_created_from_ads_ui': False, \n",
    "    'is_crosspostable': True, \n",
    "    'is_meta': False, \n",
    "    'is_original_content': False, \n",
    "    'is_reddit_media_domain': False, \n",
    "    'is_robot_indexable': True, \n",
    "    'is_self': True, \n",
    "    'is_video': False, \n",
    "    'link_flair_background_color': '', \n",
    "    'link_flair_css_class': None, \n",
    "    'link_flair_richtext': [], \n",
    "    'link_flair_text': None, \n",
    "    'link_flair_text_color': 'dark', \n",
    "    'link_flair_type': 'text', \n",
    "    'locked': False,\n",
    "    'media': None, \n",
    "    'media_embed': {}, \n",
    "    'media_only': False, \n",
    "    'name': 't3_mhj2hj', \n",
    "    'no_follow': True, \n",
    "    'num_comments': 2, \n",
    "    'num_crossposts': 0, \n",
    "    'over_18': False, \n",
    "    'parent_whitelist_status': None, \n",
    "    'permalink': '/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'pinned': False, \n",
    "    'pwls': None, \n",
    "    'quarantine': False, \n",
    "    'removed_by_category': None, \n",
    "    'retrieved_utc': 1623447663, \n",
    "    'score': 1, \n",
    "    'secure_media': None, \n",
    "    'secure_media_embed': {}, \n",
    "    'selftext': 'Los p치jaros est치n cantando, las hierbas verdes est치n brotando, y tengo alergias.  Esto es la temporada de las alergias.  Estornudo cada ma침ana cuando me despierto, y otra vez si voy afuera.  Necesito tomar medicina cada d칤a, pero no funciona tan bien. \\n\\nPor fuera, las lomas son bonitas porque son verdes y los robles tienen hojas nuevas.  Por el fin de semana,  hago caminatas pero cuando regreso a casa, necesito ducharme para remover el polen.\\n\\nCuando me jubile, voy a viajar al desierto cada a침o por toda la primavera.  No me gustar칤a quedarme aqu칤.', \n",
    "    'send_replies': True, \n",
    "    'spoiler': False, \n",
    "    'stickied': False, \n",
    "    'subreddit': 'WriteStreakES', \n",
    "    'subreddit_id': 't5_2eamt5', \n",
    "    'subreddit_subscribers': 2205, \n",
    "    'subreddit_type': 'public', \n",
    "    'suggested_sort': None, \n",
    "    'thumbnail': 'self', \n",
    "    'thumbnail_height': None, \n",
    "    'thumbnail_width': None, \n",
    "    'title': 'Streak 90: Ha llegado la primavera', \n",
    "    'top_awarded_type': None, \n",
    "    'total_awards_received': 0, \n",
    "    'treatment_tags': [], \n",
    "    'upvote_ratio': 1.0, \n",
    "    'url': 'https://www.reddit.com/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'whitelist_status': None, 'wls': None}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctx = zstd.ZstdDecompressor(max_window_size=2147483648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findURLs(phrase):\n",
    "    regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)')\n",
    "    url = re.findall(regex, phrase)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lol.com']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out\n",
    "findURLs(\"does this find https://lol.com or nytimes.com/2021/10/19/us/politics/trump-border.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname(url, uri_type='both'):\n",
    "    \"\"\"Get the host name from the url\"\"\"\n",
    "    # domain = re.compile(r\"(https?://)?(www\\.)?\")\n",
    "    # return domain.sub('', url).strip().strip('/').split('/')[0]\n",
    "    hostnames = set()\n",
    "    extracted = tldextract.extract(url)\n",
    "    subdomain, domain, suffix = extracted\n",
    "    # add both versions of domain.suffix and subdomain.domain.suffix\n",
    "    full = \"\"\n",
    "    # with subdomain\n",
    "    if len(subdomain) > 0 and len(suffix) > 0:\n",
    "        #print(f\"{subdomain}.{domain}.{suffix}\")\n",
    "        full = f\"{subdomain}.{domain}.{suffix}\"\n",
    "        if len(full) > 0:\n",
    "            full = full[4:].strip('/') if full.startswith(\"www.\") else full.strip('/')\n",
    "            # if full in gmm_news_sources: # ******* gmm_news_sources_ added here *******\n",
    "            #     return full\n",
    "            hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "            # hostnames.add(full.replace(\"www.\",\"\").strip('/'))\n",
    "    # without subdomain\n",
    "    full = f\"{domain}.{suffix}\"\n",
    "    if len(full) > 0 and len(suffix) > 0:\n",
    "        full = full[4:].strip('/') if full.startswith(\"www.\") else full.strip('/')\n",
    "        # if full in gmm_news_sources: # ******* gmm_news_sources_ added here *******\n",
    "        #     return full\n",
    "    # return \"\"\n",
    "        hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "        # hostnames.add(full.replace(\"www.\",\"\").strip('/'))\n",
    "    return hostnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nytimes.com'}\n",
      "{'nytimes.com', 'aiaia.nytimes.com'}\n",
      "{'nytimes.com'}\n"
     ]
    }
   ],
   "source": [
    "# function try out\n",
    "print(get_hostname(\"https://www.nytimes.com\"))\n",
    "print(get_hostname(\"http://www.aiaia.nytimes.com/add\"))\n",
    "print(get_hostname(\"www.nytimes.com/additional\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"realtor.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "zst_files = [\"RS_2021-01.zst\", \"RS_2021-02.zst\", \"RS_2021-03.zst\", \"RS_2021-04.zst\", \"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "# zst_files = [\"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "zst_filepath = \"E:/thesis_data/\" # D for ThinkPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_srid = dict()\n",
    "\n",
    "posts_with_urls = list()\n",
    "posts_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'baz', 'quux', 'tup_1', 'tup_2', 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "x = [[], ['foo'], ['bar', 'baz'], ['quux'], (\"tup_1\", \"tup_2\"), {1:\"one\", 2:\"two\"}]\n",
    "print(list(itertools.chain(*x)))\n",
    "# print([element for sub in x for element in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2022-02-10 00:08:52.617949\n",
      "***** Start processing for RS_2021-02.zst *****\n",
      "processed 500000 by 00:10:15\n",
      "processed 1000000 by 00:11:32\n",
      "processed 1500000 by 00:12:48\n",
      "processed 2000000 by 00:14:08\n",
      "processed 2500000 by 00:15:28\n",
      "processed 3000000 by 00:16:39\n",
      "processed 3500000 by 00:17:49\n",
      "processed 4000000 by 00:19:00\n",
      "processed 4500000 by 00:20:09\n",
      "processed 5000000 by 00:21:19\n",
      "processed 5500000 by 00:22:31\n",
      "processed 6000000 by 00:23:43\n",
      "processed 6500000 by 00:24:51\n",
      "processed 7000000 by 00:26:04\n",
      "processed 7500000 by 00:27:17\n",
      "processed 8000000 by 00:28:32\n",
      "processed 8500000 by 00:29:44\n",
      "processed 9000000 by 00:30:58\n",
      "processed 9500000 by 00:32:11\n",
      "processed 10000000 by 00:33:22\n",
      "processed 10500000 by 00:34:38\n",
      "processed 11000000 by 00:35:50\n",
      "processed 11500000 by 00:37:07\n",
      "processed 12000000 by 00:38:21\n",
      "processed 12500000 by 00:39:33\n",
      "processed 13000000 by 00:40:47\n",
      "processed 13500000 by 00:42:01\n",
      "processed 14000000 by 00:43:10\n",
      "processed 14500000 by 00:44:19\n",
      "processed 15000000 by 00:45:25\n",
      "processed 15500000 by 00:46:32\n",
      "processed 16000000 by 00:47:40\n",
      "processed 16500000 by 00:49:01\n",
      "processed 17000000 by 00:50:20\n",
      "processed 17500000 by 00:51:27\n",
      "processed 18000000 by 00:52:35\n",
      "processed 18500000 by 00:53:42\n",
      "processed 19000000 by 00:54:49\n",
      "processed 19500000 by 00:55:55\n",
      "processed 20000000 by 00:57:03\n",
      "processed 20500000 by 00:58:11\n",
      "processed 21000000 by 00:59:19\n",
      "processed 21500000 by 01:00:26\n",
      "processed 22000000 by 01:01:46\n",
      "processed 22500000 by 01:03:00\n",
      "processed 23000000 by 01:04:14\n",
      "processed 23500000 by 01:05:29\n",
      "processed 24000000 by 01:06:48\n",
      "processed 24500000 by 01:08:05\n",
      "processed 25000000 by 01:09:22\n",
      "processed 25500000 by 01:10:42\n",
      "processed 26000000 by 01:11:51\n",
      "processed 26500000 by 01:13:03\n",
      "processed 27000000 by 01:14:11\n",
      "processed 27500000 by 01:15:22\n",
      "processed 28000000 by 01:16:35\n",
      "processed 28500000 by 01:18:03\n",
      "processed 29000000 by 01:19:19\n",
      "processed 29500000 by 01:20:36\n",
      "processed 30000000 by 01:21:49\n",
      "processed 30500000 by 01:23:00\n",
      "processed 31000000 by 01:24:11\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-02.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-03.zst *****\n",
      "processed 500000 by 01:25:49\n",
      "processed 1000000 by 01:27:00\n",
      "processed 1500000 by 01:28:09\n",
      "processed 2000000 by 01:29:20\n",
      "processed 2500000 by 01:30:29\n",
      "processed 3000000 by 01:31:39\n",
      "processed 3500000 by 01:32:49\n",
      "processed 4000000 by 01:33:58\n",
      "processed 4500000 by 01:35:07\n",
      "processed 5000000 by 01:36:17\n",
      "processed 5500000 by 01:37:26\n",
      "processed 6000000 by 01:38:36\n",
      "processed 6500000 by 01:39:44\n",
      "processed 7000000 by 01:40:52\n",
      "processed 7500000 by 01:42:01\n",
      "processed 8000000 by 01:43:09\n",
      "processed 8500000 by 01:44:18\n",
      "processed 9000000 by 01:45:26\n",
      "processed 9500000 by 01:46:35\n",
      "processed 10000000 by 01:47:45\n",
      "processed 10500000 by 01:48:51\n",
      "processed 11000000 by 01:49:56\n",
      "processed 11500000 by 01:51:03\n",
      "processed 12000000 by 01:52:10\n",
      "processed 12500000 by 01:53:17\n",
      "processed 13000000 by 01:54:25\n",
      "processed 13500000 by 01:55:33\n",
      "processed 14000000 by 01:56:42\n",
      "processed 14500000 by 01:57:49\n",
      "processed 15000000 by 01:58:57\n",
      "processed 15500000 by 02:00:04\n",
      "processed 16000000 by 02:01:13\n",
      "processed 16500000 by 02:02:21\n",
      "processed 17000000 by 02:03:29\n",
      "processed 17500000 by 02:04:38\n",
      "processed 18000000 by 02:05:48\n",
      "processed 18500000 by 02:06:56\n",
      "processed 19000000 by 02:08:05\n",
      "processed 19500000 by 02:09:13\n",
      "processed 20000000 by 02:10:21\n",
      "processed 20500000 by 02:11:29\n",
      "processed 21000000 by 02:12:36\n",
      "processed 21500000 by 02:13:44\n",
      "processed 22000000 by 02:14:50\n",
      "processed 22500000 by 02:15:59\n",
      "processed 23000000 by 02:17:06\n",
      "processed 23500000 by 02:18:15\n",
      "processed 24000000 by 02:19:23\n",
      "processed 24500000 by 02:20:32\n",
      "processed 25000000 by 02:21:40\n",
      "processed 25500000 by 02:22:49\n",
      "processed 26000000 by 02:23:57\n",
      "processed 26500000 by 02:25:07\n",
      "processed 27000000 by 02:26:16\n",
      "processed 27500000 by 02:27:27\n",
      "processed 28000000 by 02:28:35\n",
      "processed 28500000 by 02:29:42\n",
      "processed 29000000 by 02:30:50\n",
      "processed 29500000 by 02:31:59\n",
      "processed 30000000 by 02:33:07\n",
      "processed 30500000 by 02:34:16\n",
      "processed 31000000 by 02:35:25\n",
      "processed 31500000 by 02:36:34\n",
      "processed 32000000 by 02:37:43\n",
      "processed 32500000 by 02:38:51\n",
      "processed 33000000 by 02:39:59\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-03.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-04.zst *****\n",
      "processed 500000 by 02:41:13\n",
      "processed 1000000 by 02:42:23\n",
      "processed 1500000 by 02:43:33\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "for zst_file in zst_files[1:]:\n",
    "    ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    subreddit_ns = defaultdict(Counter)\n",
    "    print(\"***** Start processing for {} *****\".format(zst_file))\n",
    "    with open(zst_filepath+zst_file, 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "        with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            url_of_our_ns = False \n",
    "            # ^this is to keep track if this post has ns url that we care about.\n",
    "            # if so, add to posts_with_urls once.\n",
    "            for d in text_stream:\n",
    "                line = json.loads(d)\n",
    "                subreddit, subreddit_id = line['subreddit'], line['subreddit_id']\n",
    "                if subreddit not in subreddit_srid:\n",
    "                    subreddit_srid[subreddit] = subreddit_id\n",
    "                URLs = findURLs(line['url']) + findURLs(line['selftext'])                \n",
    "                hostnames = [get_hostname(url) for url in URLs]\n",
    "                URLs = Counter([element for sub in hostnames for element in sub])\n",
    "                # print(URLs)\n",
    "                # URLs = itertools.chain(*hostnames)\n",
    "                # if len(URLs) > 10: print(line['selftext'])\n",
    "                for url in URLs:\n",
    "                    if url in gmm_news_sources: # instead of the full  news_sources\n",
    "                        if not url_of_our_ns:\n",
    "                            # posts_with_urls.append(line)\n",
    "                            url_of_our_ns = True\n",
    "                        ns_subreddit[url][subreddit] += URLs[url] # 1\n",
    "                        subreddit_ns[subreddit][url] += URLs[url] # 1\n",
    "                        # break\n",
    "                        # print(f\"ns_subreddit: {ns_subreddit}\")\n",
    "                url_of_our_ns = False\n",
    "                counter += 1\n",
    "                if counter%500000 == 0: \n",
    "                    print(\"processed {} by {}\".format(counter, str(datetime.datetime.now())[11:19]))\n",
    "                \n",
    "    \n",
    "    print(\"-------------------------------- Done reading, will write files now --------------------------------\")\n",
    "    \n",
    "    # write into files separated by months\n",
    "    with open(\"ns_subreddit_{}.json\".format(zst_file[3:10]), \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(ns_subreddit, outfile, indent=4)\n",
    "        \n",
    "    with open(\"subreddit_ns_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile1:\n",
    "        json.dump(subreddit_ns, outfile1, indent=4)\n",
    "        \n",
    "    with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "        json.dump(subreddit_srid, infile_srid, indent=4)\n",
    "        \n",
    "    # with open(\"E:\\\\thesis_data\\\\posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "        # json.dump(posts_with_urls, outfile2, indent=4)\n",
    "        \n",
    "    # ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    # subreddit_ns = defaultdict(Counter)\n",
    "    # subreddit_srid = dict()\n",
    "    posts_with_urls = list()\n",
    "    counter = 0\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------- Done processing for {} --------------------------------\".format(zst_file))\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"tunein.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "#     json.dump(subreddit_srid, infile_srid)\n",
    "        \n",
    "# with open(\"D:/thesis_data/posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "#     json.dump(posts_with_urls, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-03'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zst_file[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of posts read from \n",
    "1. January 2021:\n",
    "2. February 2021: 31,161,912\n",
    "3. March 2021: 33,0061,03\n",
    "4. April 2021:\n",
    "5. May 2021: >36M\n",
    "6. June 2021: >34M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"subreddit_srid_{}.json\".format(\"2021-04\"), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "    json.dump(subreddit_srid, infile_srid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588719"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_with_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3686"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ns_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3686 news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddit_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20755 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811504"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_with_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ns_subreddit.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(ns_subreddit, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subreddit_ns.json\", \"w\", encoding = \"utf-8\") as outfile1:\n",
    "    json.dump(subreddit_ns, outfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"posts_with_urls\", \"w\", encoding = \"utf-8\") as outfile2:\n",
    "    json.dump(posts_with_urls, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31616206"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"counter_april21.json\", \"w\", encoding = \"utf-8\") as counterfile:\n",
    "    json.dump(counter, counterfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting subreddit names and ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_id = defaultdict(set)\n",
    "id_subreddit = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2021-11-05 01:59:23.609969\n",
      "at 500000\n",
      "at 1000000\n",
      "at 1500000\n",
      "at 2000000\n",
      "at 2500000\n",
      "at 3000000\n",
      "at 3500000\n",
      "at 4000000\n",
      "at 4500000\n",
      "at 5000000\n",
      "at 5500000\n",
      "at 6000000\n",
      "at 6500000\n",
      "at 7000000\n",
      "at 7500000\n",
      "at 8000000\n",
      "at 8500000\n",
      "at 9000000\n",
      "at 9500000\n",
      "at 10000000\n",
      "at 10500000\n",
      "at 11000000\n",
      "at 11500000\n",
      "at 12000000\n",
      "at 12500000\n",
      "at 13000000\n",
      "at 13500000\n",
      "at 14000000\n",
      "at 14500000\n",
      "at 15000000\n",
      "at 15500000\n",
      "at 16000000\n",
      "at 16500000\n",
      "at 17000000\n",
      "at 17500000\n",
      "at 18000000\n",
      "at 18500000\n",
      "at 19000000\n",
      "at 19500000\n",
      "at 20000000\n",
      "at 20500000\n",
      "at 21000000\n",
      "at 21500000\n",
      "at 22000000\n",
      "at 22500000\n",
      "at 23000000\n",
      "at 23500000\n",
      "at 24000000\n",
      "at 24500000\n",
      "at 25000000\n",
      "at 25500000\n",
      "at 26000000\n",
      "at 26500000\n",
      "at 27000000\n",
      "at 27500000\n",
      "at 28000000\n",
      "at 28500000\n",
      "at 29000000\n",
      "at 29500000\n",
      "at 30000000\n",
      "at 30500000\n",
      "at 31000000\n",
      "at 31500000\n",
      "finish time: 2021-11-05 03:35:19.988106\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "with open(\"D://Wellesley/F21/thesis_zst_data/RS_2021-04.zst\", 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "    with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "        text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "        for d in text_stream:\n",
    "            line = json.loads(d)\n",
    "            sr, sr_id = line['subreddit'], line['subreddit_id']\n",
    "            subreddit_id[sr].add(sr_id)\n",
    "            id_subreddit[sr_id].add(sr)\n",
    "#             URLs = findURLs(line['url']) + findURLs(line['selftext'])\n",
    "#             URLs = [get_hostname(url) for url in URLs]\n",
    "#             # print(\"URLs:\", URLs)\n",
    "#             # if len(URLs) > 10: print(line['selftext'])\n",
    "#             for url in URLs:\n",
    "#                 if url in news_sources:\n",
    "#                     posts_with_urls.append(line)\n",
    "#                     ns_subreddit[url][subreddit] += 1\n",
    "#                     subreddit_ns[subreddit][url] += 1\n",
    "            counter += 1\n",
    "            if counter%500000 == 0: print(f\"at {counter}\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639811"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639811"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddit_id:\n",
    "    if len(subreddit_id[s]) != 1:\n",
    "        print(f\"{s} is invalid, length {len(subreddit_id[s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in id_subreddit:\n",
    "    if len(id_subreddit[s]) != 1:\n",
    "        print(f\"{s} is invalid, length {len(id_subreddit[s])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make all values to be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddit_id:\n",
    "    subreddit_id[s] = list(subreddit_id[s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in id_subreddit:\n",
    "    id_subreddit[i] = list(id_subreddit[i])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the same number of `id`s and `subreddit`s. Good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subreddit_id.json\", \"w\", encoding = \"utf-8\") as outfile_si:\n",
    "    json.dump(subreddit_id, outfile_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"id_subreddit.json\", \"w\", encoding = \"utf-8\") as outfile_is:\n",
    "    json.dump(id_subreddit, outfile_is)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary look at subreddits for 6k news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zstandard\n",
      "  Downloading zstandard-0.16.0-cp39-cp39-win_amd64.whl (733 kB)\n",
      "Installing collected packages: zstandard\n",
      "Successfully installed zstandard-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import io\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import datetime, time\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 19:35:57.025969\n",
      "19:35:57\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print(str(datetime.datetime.now())[11:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gm_intersection.json\", \"r\") as infile:\n",
    "    news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42477"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['websterprogresstimes.com',\n",
       " 'cordeledispatch.com',\n",
       " 'k12.wv.us',\n",
       " 'ukconstructionmedia.co.uk',\n",
       " 'dylanpaulus.com',\n",
       " 'arktimes.com',\n",
       " 'asiafoodjournal.com',\n",
       " 'corydontimes.com',\n",
       " 'stuttgartdailyleader.com',\n",
       " 'artrockermagazine.com']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `gmm` instead\n",
    "\n",
    "Since there are *A LOT* of news sources in `gm_intersect` which is the intersection of gdelt and muckrack, let's instead use `gmm_intersect` which is the intersection of gdelt, muckrack,and mbfc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gmm_intersection.json\", \"r\") as infile:\n",
    "    gmm_news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gmm_news_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open reddit data from April 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://arxiv.org/pdf/2001.08435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an entry of data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'all_awardings': [], \n",
    "    'allow_live_comments': False, \n",
    "    'archived': False, \n",
    "    'author': 'elanglohablante9805', \n",
    "    'author_created_utc': 1609519842, \n",
    "    'author_flair_background_color': '#ffb000', \n",
    "    'author_flair_css_class': None, \n",
    "    'author_flair_richtext': [], \n",
    "    'author_flair_template_id': '4f908eaa-9664-11ea-a567-0ed46a42aec3', \n",
    "    'author_flair_text': 'Historiador 游닆 | 80-Day Streak 游댠', \n",
    "    'author_flair_text_color': 'dark', \n",
    "    'author_flair_type': 'text', \n",
    "    'author_fullname': 't2_9lr431i4', \n",
    "    'author_patreon_flair': False, \n",
    "    'author_premium': False, \n",
    "    'can_gild': True, \n",
    "    'category': None, \n",
    "    'content_categories': None, \n",
    "    'contest_mode': False, \n",
    "    'created_utc': 1617235201, \n",
    "    'discussion_type': None, \n",
    "    'distinguished': None, \n",
    "    'domain': 'self.WriteStreakES', \n",
    "    'edited': False, \n",
    "    'gilded': 0, \n",
    "    'gildings': {}, \n",
    "    'hidden': False, \n",
    "    'hide_score': False, \n",
    "    'id': 'mhj2hj', \n",
    "    'is_created_from_ads_ui': False, \n",
    "    'is_crosspostable': True, \n",
    "    'is_meta': False, \n",
    "    'is_original_content': False, \n",
    "    'is_reddit_media_domain': False, \n",
    "    'is_robot_indexable': True, \n",
    "    'is_self': True, \n",
    "    'is_video': False, \n",
    "    'link_flair_background_color': '', \n",
    "    'link_flair_css_class': None, \n",
    "    'link_flair_richtext': [], \n",
    "    'link_flair_text': None, \n",
    "    'link_flair_text_color': 'dark', \n",
    "    'link_flair_type': 'text', \n",
    "    'locked': False,\n",
    "    'media': None, \n",
    "    'media_embed': {}, \n",
    "    'media_only': False, \n",
    "    'name': 't3_mhj2hj', \n",
    "    'no_follow': True, \n",
    "    'num_comments': 2, \n",
    "    'num_crossposts': 0, \n",
    "    'over_18': False, \n",
    "    'parent_whitelist_status': None, \n",
    "    'permalink': '/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'pinned': False, \n",
    "    'pwls': None, \n",
    "    'quarantine': False, \n",
    "    'removed_by_category': None, \n",
    "    'retrieved_utc': 1623447663, \n",
    "    'score': 1, \n",
    "    'secure_media': None, \n",
    "    'secure_media_embed': {}, \n",
    "    'selftext': 'Los p치jaros est치n cantando, las hierbas verdes est치n brotando, y tengo alergias.  Esto es la temporada de las alergias.  Estornudo cada ma침ana cuando me despierto, y otra vez si voy afuera.  Necesito tomar medicina cada d칤a, pero no funciona tan bien. \\n\\nPor fuera, las lomas son bonitas porque son verdes y los robles tienen hojas nuevas.  Por el fin de semana,  hago caminatas pero cuando regreso a casa, necesito ducharme para remover el polen.\\n\\nCuando me jubile, voy a viajar al desierto cada a침o por toda la primavera.  No me gustar칤a quedarme aqu칤.', \n",
    "    'send_replies': True, \n",
    "    'spoiler': False, \n",
    "    'stickied': False, \n",
    "    'subreddit': 'WriteStreakES', \n",
    "    'subreddit_id': 't5_2eamt5', \n",
    "    'subreddit_subscribers': 2205, \n",
    "    'subreddit_type': 'public', \n",
    "    'suggested_sort': None, \n",
    "    'thumbnail': 'self', \n",
    "    'thumbnail_height': None, \n",
    "    'thumbnail_width': None, \n",
    "    'title': 'Streak 90: Ha llegado la primavera', \n",
    "    'top_awarded_type': None, \n",
    "    'total_awards_received': 0, \n",
    "    'treatment_tags': [], \n",
    "    'upvote_ratio': 1.0, \n",
    "    'url': 'https://www.reddit.com/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'whitelist_status': None, 'wls': None}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctx = zstd.ZstdDecompressor(max_window_size=2147483648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findURLs(phrase):\n",
    "    regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)')\n",
    "    url = re.findall(regex, phrase)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c1e0be9939bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# try out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindURLs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"does this find https://lol.com or nytimes.com/2021/10/19/us/politics/trump-border.html or https://nytimes.com/2021/10/19/us/politics/trump-border.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-3c932e692bad>\u001b[0m in \u001b[0;36mfindURLs\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfindURLs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# try out\n",
    "findURLs(\"does this find https://lol.com or nytimes.com/2021/10/19/us/politics/trump-border.html or https://nytimes.com/2021/10/19/us/politics/trump-border.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname(url, uri_type='both'):\n",
    "    \"\"\"Get the host name from the url\"\"\"\n",
    "    # domain = re.compile(r\"(https?://)?(www\\.)?\")\n",
    "    # return domain.sub('', url).strip().strip('/').split('/')[0]\n",
    "    hostnames = set()\n",
    "    extracted = tldextract.extract(url)\n",
    "    subdomain, domain, suffix = extracted\n",
    "    # add both versions of domain.suffix and subdomain.domain.suffix\n",
    "    full = \"\"\n",
    "    # with subdomain\n",
    "    if len(subdomain) > 0 and len(suffix) > 0:\n",
    "        #print(f\"{subdomain}.{domain}.{suffix}\")\n",
    "        full = f\"{subdomain}.{domain}.{suffix}\"\n",
    "        if len(full) > 0:\n",
    "            full = full[4:].strip('/') if full.startswith(\"www.\") else full.strip('/')\n",
    "            # if full in gmm_news_sources: # ******* gmm_news_sources_ added here *******\n",
    "            #     return full\n",
    "            hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "            # hostnames.add(full.replace(\"www.\",\"\").strip('/'))\n",
    "    # without subdomain\n",
    "    full = f\"{domain}.{suffix}\"\n",
    "    if len(full) > 0 and len(suffix) > 0:\n",
    "        full = full[4:].strip('/') if full.startswith(\"www.\") else full.strip('/')\n",
    "        # if full in gmm_news_sources: # ******* gmm_news_sources_ added here *******\n",
    "        #     return full\n",
    "    # return \"\"\n",
    "        hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "        # hostnames.add(full.replace(\"www.\",\"\").strip('/'))\n",
    "    return hostnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nytimes.com'}\n",
      "{'aiaia.nytimes.com', 'nytimes.com'}\n",
      "{'nytimes.com'}\n"
     ]
    }
   ],
   "source": [
    "# function try out\n",
    "print(get_hostname(\"https://www.nytimes.com\"))\n",
    "print(get_hostname(\"http://www.aiaia.nytimes.com/add\"))\n",
    "print(get_hostname(\"www.nytimes.com/additional\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"realtor.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zst_files = [\"RS_2021-01.zst\", \"RS_2021-02.zst\", \"RS_2021-03.zst\", \"RS_2021-04.zst\", \"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "# zst_files = [\"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "zst_filepath = \"E:/thesis_data/\" # D for ThinkPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_srid = dict()\n",
    "\n",
    "posts_with_urls = list()\n",
    "posts_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'baz', 'quux', 'tup_1', 'tup_2', 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "x = [[], ['foo'], ['bar', 'baz'], ['quux'], (\"tup_1\", \"tup_2\"), {1:\"one\", 2:\"two\"}]\n",
    "print(list(itertools.chain(*x)))\n",
    "# print([element for sub in x for element in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2022-02-19 00:04:37.973984\n",
      "***** Start processing for RS_2021-04.zst *****\n",
      "processed 500000 by 00:06:32\n",
      "processed 1000000 by 00:08:25\n",
      "processed 1500000 by 00:10:14\n",
      "processed 2000000 by 00:12:05\n",
      "processed 2500000 by 00:13:55\n",
      "processed 3000000 by 00:15:48\n",
      "processed 3500000 by 00:17:41\n",
      "processed 4000000 by 00:19:33\n",
      "processed 4500000 by 00:21:26\n",
      "processed 5000000 by 00:23:58\n",
      "processed 5500000 by 00:25:50\n",
      "processed 6000000 by 00:27:43\n",
      "processed 6500000 by 00:29:35\n",
      "processed 7000000 by 00:31:29\n",
      "processed 7500000 by 00:33:19\n",
      "processed 8000000 by 00:35:12\n",
      "processed 8500000 by 00:37:08\n",
      "processed 9000000 by 00:39:01\n",
      "processed 9500000 by 00:40:54\n",
      "processed 10000000 by 00:42:47\n",
      "processed 10500000 by 00:44:38\n",
      "processed 11000000 by 00:47:14\n",
      "processed 11500000 by 00:49:15\n",
      "processed 12000000 by 00:51:10\n",
      "processed 12500000 by 00:53:04\n",
      "processed 13000000 by 00:54:58\n",
      "processed 13500000 by 00:56:51\n",
      "processed 14000000 by 00:58:44\n",
      "processed 14500000 by 01:00:37\n",
      "processed 15000000 by 01:02:31\n",
      "processed 15500000 by 01:04:06\n",
      "processed 16000000 by 01:05:39\n",
      "processed 16500000 by 01:07:33\n",
      "processed 17000000 by 01:09:30\n",
      "processed 17500000 by 01:11:28\n",
      "processed 18000000 by 01:13:18\n",
      "processed 18500000 by 01:15:05\n",
      "processed 19000000 by 01:17:03\n",
      "processed 19500000 by 01:19:17\n",
      "processed 20000000 by 01:21:36\n",
      "processed 20500000 by 01:24:07\n",
      "processed 21000000 by 01:26:36\n",
      "processed 21500000 by 01:29:09\n",
      "processed 22000000 by 01:31:42\n",
      "processed 22500000 by 01:34:16\n",
      "processed 23000000 by 01:36:49\n",
      "processed 23500000 by 01:39:22\n",
      "processed 24000000 by 01:41:53\n",
      "processed 24500000 by 01:44:10\n",
      "processed 25000000 by 01:46:25\n",
      "processed 25500000 by 01:48:40\n",
      "processed 26000000 by 01:50:57\n",
      "processed 26500000 by 01:53:13\n",
      "processed 27000000 by 01:55:29\n",
      "processed 27500000 by 01:57:42\n",
      "processed 28000000 by 02:00:02\n",
      "processed 28500000 by 02:02:20\n",
      "processed 29000000 by 02:04:38\n",
      "processed 29500000 by 02:06:54\n",
      "processed 30000000 by 02:09:13\n",
      "processed 30500000 by 02:11:29\n",
      "processed 31000000 by 02:13:46\n",
      "processed 31500000 by 02:16:06\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-04.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-05.zst *****\n",
      "processed 500000 by 02:18:58\n",
      "processed 1000000 by 02:21:17\n",
      "processed 1500000 by 02:23:35\n",
      "processed 2000000 by 02:25:52\n",
      "processed 2500000 by 02:28:07\n",
      "processed 3000000 by 02:30:23\n",
      "processed 3500000 by 02:32:39\n",
      "processed 4000000 by 02:34:57\n",
      "processed 4500000 by 02:37:12\n",
      "processed 5000000 by 02:39:29\n",
      "processed 5500000 by 02:41:47\n",
      "processed 6000000 by 02:44:02\n",
      "processed 6500000 by 02:46:18\n",
      "processed 7000000 by 02:48:35\n",
      "processed 7500000 by 02:50:53\n",
      "processed 8000000 by 02:53:12\n",
      "processed 8500000 by 02:55:34\n",
      "processed 9000000 by 02:57:55\n",
      "processed 9500000 by 03:00:07\n",
      "processed 10000000 by 03:02:20\n",
      "processed 10500000 by 03:04:32\n",
      "processed 11000000 by 03:06:50\n",
      "processed 11500000 by 03:09:04\n",
      "processed 12000000 by 03:11:18\n",
      "processed 12500000 by 03:13:30\n",
      "processed 13000000 by 03:15:43\n",
      "processed 13500000 by 03:17:59\n",
      "processed 14000000 by 03:20:17\n",
      "processed 14500000 by 03:22:33\n",
      "processed 15000000 by 03:24:50\n",
      "processed 15500000 by 03:27:03\n",
      "processed 16000000 by 03:29:22\n",
      "processed 16500000 by 03:31:36\n",
      "processed 17000000 by 03:33:53\n",
      "processed 17500000 by 03:36:08\n",
      "processed 18000000 by 03:38:22\n",
      "processed 18500000 by 03:40:38\n",
      "processed 19000000 by 03:42:47\n",
      "processed 19500000 by 03:45:01\n",
      "processed 20000000 by 03:47:12\n",
      "processed 20500000 by 03:49:23\n",
      "processed 21000000 by 03:51:35\n",
      "processed 21500000 by 03:53:45\n",
      "processed 22000000 by 03:55:57\n",
      "processed 22500000 by 03:58:09\n",
      "processed 23000000 by 04:00:21\n",
      "processed 23500000 by 04:02:39\n",
      "processed 24000000 by 04:04:52\n",
      "processed 24500000 by 04:07:07\n",
      "processed 25000000 by 04:09:21\n",
      "processed 25500000 by 04:11:33\n",
      "processed 26000000 by 04:13:47\n",
      "processed 26500000 by 04:15:59\n",
      "processed 27000000 by 04:18:13\n",
      "processed 27500000 by 04:20:26\n",
      "processed 28000000 by 04:22:40\n",
      "processed 28500000 by 04:24:52\n",
      "processed 29000000 by 04:27:06\n",
      "processed 29500000 by 04:29:22\n",
      "processed 30000000 by 04:31:34\n",
      "processed 30500000 by 04:33:51\n",
      "processed 31000000 by 04:36:09\n",
      "processed 31500000 by 04:38:26\n",
      "processed 32000000 by 04:40:43\n",
      "processed 32500000 by 04:43:01\n",
      "processed 33000000 by 04:45:19\n",
      "processed 33500000 by 04:47:32\n",
      "processed 34000000 by 04:49:49\n",
      "processed 34500000 by 04:52:05\n",
      "processed 35000000 by 04:54:24\n",
      "processed 35500000 by 04:56:37\n",
      "processed 36000000 by 04:58:53\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-05.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-06.zst *****\n",
      "processed 500000 by 05:02:43\n",
      "processed 1000000 by 05:05:04\n",
      "processed 1500000 by 05:07:21\n",
      "processed 2000000 by 05:09:41\n",
      "processed 2500000 by 05:12:00\n",
      "processed 3000000 by 05:14:17\n",
      "processed 3500000 by 05:16:37\n",
      "processed 4000000 by 05:18:55\n",
      "processed 4500000 by 05:21:14\n",
      "processed 5000000 by 05:23:30\n",
      "processed 5500000 by 05:25:47\n",
      "processed 6000000 by 05:28:04\n",
      "processed 6500000 by 05:30:18\n",
      "processed 7000000 by 05:32:37\n",
      "processed 7500000 by 05:34:58\n",
      "processed 8000000 by 05:37:59\n",
      "processed 8500000 by 05:41:00\n",
      "processed 9000000 by 05:44:00\n",
      "processed 9500000 by 05:46:56\n",
      "processed 10000000 by 05:49:52\n",
      "processed 10500000 by 05:52:55\n",
      "processed 11000000 by 05:55:51\n",
      "processed 11500000 by 05:58:46\n",
      "processed 12000000 by 06:01:46\n",
      "processed 12500000 by 06:04:43\n",
      "processed 13000000 by 06:07:49\n",
      "processed 13500000 by 06:10:47\n",
      "processed 14000000 by 06:13:44\n",
      "processed 14500000 by 06:16:41\n",
      "processed 15000000 by 06:19:35\n",
      "processed 15500000 by 06:22:29\n",
      "processed 16000000 by 06:25:23\n",
      "processed 16500000 by 06:28:28\n",
      "processed 17000000 by 06:31:23\n",
      "processed 17500000 by 06:34:20\n",
      "processed 18000000 by 06:37:18\n",
      "processed 18500000 by 06:40:14\n",
      "processed 19000000 by 06:43:10\n",
      "processed 19500000 by 06:46:02\n",
      "processed 20000000 by 06:49:03\n",
      "processed 20500000 by 06:51:58\n",
      "processed 21000000 by 06:54:52\n",
      "processed 21500000 by 06:57:41\n",
      "processed 22000000 by 07:00:31\n",
      "processed 22500000 by 07:03:25\n",
      "processed 23000000 by 07:06:14\n",
      "processed 23500000 by 07:09:15\n",
      "processed 24000000 by 07:12:08\n",
      "processed 24500000 by 07:15:03\n",
      "processed 25000000 by 07:17:56\n",
      "processed 25500000 by 07:20:51\n",
      "processed 26000000 by 07:23:46\n",
      "processed 26500000 by 07:26:47\n",
      "processed 27000000 by 07:29:52\n",
      "processed 27500000 by 19:59:23\n",
      "processed 28000000 by 20:01:10\n",
      "processed 28500000 by 20:03:43\n",
      "processed 29000000 by 20:06:10\n",
      "processed 29500000 by 20:08:36\n",
      "processed 30000000 by 20:10:57\n",
      "processed 30500000 by 20:13:08\n",
      "processed 31000000 by 20:15:24\n",
      "processed 31500000 by 20:17:35\n",
      "processed 32000000 by 20:19:46\n",
      "processed 32500000 by 20:21:58\n",
      "processed 33000000 by 20:24:08\n",
      "processed 33500000 by 20:26:17\n",
      "processed 34000000 by 20:28:27\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-06.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "finish time: 2022-02-19 20:29:02.106470\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "for zst_file in zst_files[3:]:\n",
    "    ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    subreddit_ns = defaultdict(Counter)\n",
    "    print(\"***** Start processing for {} *****\".format(zst_file))\n",
    "    with open(zst_filepath+zst_file, 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "        with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            url_of_our_ns = False \n",
    "            # ^this is to keep track if this post has ns url that we care about.\n",
    "            # if so, add to posts_with_urls once.\n",
    "            for d in text_stream:\n",
    "                line = json.loads(d)\n",
    "                subreddit, subreddit_id = line['subreddit'], line['subreddit_id']\n",
    "                if subreddit not in subreddit_srid:\n",
    "                    subreddit_srid[subreddit] = subreddit_id\n",
    "                URLs = findURLs(line['url']) + findURLs(line['selftext'])                \n",
    "                hostnames = [get_hostname(url) for url in URLs]\n",
    "                URLs = Counter([element for sub in hostnames for element in sub])\n",
    "                # print(URLs)\n",
    "                # URLs = itertools.chain(*hostnames)\n",
    "                # if len(URLs) > 10: print(line['selftext'])\n",
    "                for url in URLs:\n",
    "                    if url in gmm_news_sources: # instead of the full  news_sources\n",
    "                        if not url_of_our_ns:\n",
    "                            # posts_with_urls.append(line)\n",
    "                            url_of_our_ns = True\n",
    "                        ns_subreddit[url][subreddit] += URLs[url] # 1\n",
    "                        subreddit_ns[subreddit][url] += URLs[url] # 1\n",
    "                        # break\n",
    "                        # print(f\"ns_subreddit: {ns_subreddit}\")\n",
    "                url_of_our_ns = False\n",
    "                counter += 1\n",
    "                if counter%500000 == 0: \n",
    "                    print(\"processed {} by {}\".format(counter, str(datetime.datetime.now())[11:19]))\n",
    "                \n",
    "    \n",
    "    print(\"-------------------------------- Done reading, will write files now --------------------------------\")\n",
    "    \n",
    "    # write into files separated by months\n",
    "    with open(\"ns_subreddit_{}.json\".format(zst_file[3:10]), \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(ns_subreddit, outfile, indent=4)\n",
    "        \n",
    "    with open(\"subreddit_ns_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile1:\n",
    "        json.dump(subreddit_ns, outfile1, indent=4)\n",
    "        \n",
    "    with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "        json.dump(subreddit_srid, infile_srid, indent=4)\n",
    "        \n",
    "    # with open(\"E:\\\\thesis_data\\\\posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "        # json.dump(posts_with_urls, outfile2, indent=4)\n",
    "        \n",
    "    # ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    # subreddit_ns = defaultdict(Counter)\n",
    "    # subreddit_srid = dict()\n",
    "    posts_with_urls = list()\n",
    "    counter = 0\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------- Done processing for {} --------------------------------\".format(zst_file))\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

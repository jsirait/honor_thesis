{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary look at subreddits for 6k news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zstandard\n",
      "  Downloading zstandard-0.16.0-cp39-cp39-win_amd64.whl (733 kB)\n",
      "Installing collected packages: zstandard\n",
      "Successfully installed zstandard-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import io\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import datetime, time\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-09 08:08:51.982966\n",
      "08:08:51\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print(str(datetime.datetime.now())[11:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gm_intersection.json\", \"r\") as infile:\n",
    "    news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42477"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['websterprogresstimes.com',\n",
       " 'cordeledispatch.com',\n",
       " 'k12.wv.us',\n",
       " 'ukconstructionmedia.co.uk',\n",
       " 'dylanpaulus.com',\n",
       " 'arktimes.com',\n",
       " 'asiafoodjournal.com',\n",
       " 'corydontimes.com',\n",
       " 'stuttgartdailyleader.com',\n",
       " 'artrockermagazine.com']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `gmm` instead\n",
    "\n",
    "Since there are *A LOT* of news sources in `gm_intersect` which is the intersection of gdelt and muckrack, let's instead use `gmm_intersect` which is the intersection of gdelt, muckrack,and mbfc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Wellesley\\\\F21\\\\thesis\\\\data\\\\gmm_intersection.json\", \"r\") as infile:\n",
    "    gmm_news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gmm_news_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open reddit data from April 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://arxiv.org/pdf/2001.08435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an entry of data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'all_awardings': [], \n",
    "    'allow_live_comments': False, \n",
    "    'archived': False, \n",
    "    'author': 'elanglohablante9805', \n",
    "    'author_created_utc': 1609519842, \n",
    "    'author_flair_background_color': '#ffb000', \n",
    "    'author_flair_css_class': None, \n",
    "    'author_flair_richtext': [], \n",
    "    'author_flair_template_id': '4f908eaa-9664-11ea-a567-0ed46a42aec3', \n",
    "    'author_flair_text': 'Historiador 游닆 | 80-Day Streak 游댠', \n",
    "    'author_flair_text_color': 'dark', \n",
    "    'author_flair_type': 'text', \n",
    "    'author_fullname': 't2_9lr431i4', \n",
    "    'author_patreon_flair': False, \n",
    "    'author_premium': False, \n",
    "    'can_gild': True, \n",
    "    'category': None, \n",
    "    'content_categories': None, \n",
    "    'contest_mode': False, \n",
    "    'created_utc': 1617235201, \n",
    "    'discussion_type': None, \n",
    "    'distinguished': None, \n",
    "    'domain': 'self.WriteStreakES', \n",
    "    'edited': False, \n",
    "    'gilded': 0, \n",
    "    'gildings': {}, \n",
    "    'hidden': False, \n",
    "    'hide_score': False, \n",
    "    'id': 'mhj2hj', \n",
    "    'is_created_from_ads_ui': False, \n",
    "    'is_crosspostable': True, \n",
    "    'is_meta': False, \n",
    "    'is_original_content': False, \n",
    "    'is_reddit_media_domain': False, \n",
    "    'is_robot_indexable': True, \n",
    "    'is_self': True, \n",
    "    'is_video': False, \n",
    "    'link_flair_background_color': '', \n",
    "    'link_flair_css_class': None, \n",
    "    'link_flair_richtext': [], \n",
    "    'link_flair_text': None, \n",
    "    'link_flair_text_color': 'dark', \n",
    "    'link_flair_type': 'text', \n",
    "    'locked': False,\n",
    "    'media': None, \n",
    "    'media_embed': {}, \n",
    "    'media_only': False, \n",
    "    'name': 't3_mhj2hj', \n",
    "    'no_follow': True, \n",
    "    'num_comments': 2, \n",
    "    'num_crossposts': 0, \n",
    "    'over_18': False, \n",
    "    'parent_whitelist_status': None, \n",
    "    'permalink': '/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'pinned': False, \n",
    "    'pwls': None, \n",
    "    'quarantine': False, \n",
    "    'removed_by_category': None, \n",
    "    'retrieved_utc': 1623447663, \n",
    "    'score': 1, \n",
    "    'secure_media': None, \n",
    "    'secure_media_embed': {}, \n",
    "    'selftext': 'Los p치jaros est치n cantando, las hierbas verdes est치n brotando, y tengo alergias.  Esto es la temporada de las alergias.  Estornudo cada ma침ana cuando me despierto, y otra vez si voy afuera.  Necesito tomar medicina cada d칤a, pero no funciona tan bien. \\n\\nPor fuera, las lomas son bonitas porque son verdes y los robles tienen hojas nuevas.  Por el fin de semana,  hago caminatas pero cuando regreso a casa, necesito ducharme para remover el polen.\\n\\nCuando me jubile, voy a viajar al desierto cada a침o por toda la primavera.  No me gustar칤a quedarme aqu칤.', \n",
    "    'send_replies': True, \n",
    "    'spoiler': False, \n",
    "    'stickied': False, \n",
    "    'subreddit': 'WriteStreakES', \n",
    "    'subreddit_id': 't5_2eamt5', \n",
    "    'subreddit_subscribers': 2205, \n",
    "    'subreddit_type': 'public', \n",
    "    'suggested_sort': None, \n",
    "    'thumbnail': 'self', \n",
    "    'thumbnail_height': None, \n",
    "    'thumbnail_width': None, \n",
    "    'title': 'Streak 90: Ha llegado la primavera', \n",
    "    'top_awarded_type': None, \n",
    "    'total_awards_received': 0, \n",
    "    'treatment_tags': [], \n",
    "    'upvote_ratio': 1.0, \n",
    "    'url': 'https://www.reddit.com/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'whitelist_status': None, 'wls': None}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctx = zstd.ZstdDecompressor(max_window_size=2147483648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findURLs(phrase):\n",
    "    regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)')\n",
    "    url = re.findall(regex, phrase)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lol.com']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out\n",
    "findURLs(\"does this find https://lol.com or nytimes.com/2021/10/19/us/politics/trump-border.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname(url, uri_type='both'):\n",
    "    \"\"\"Get the host name from the url\"\"\"\n",
    "    # domain = re.compile(r\"(https?://)?(www\\.)?\")\n",
    "    # return domain.sub('', url).strip().strip('/').split('/')[0]\n",
    "    hostnames = set()\n",
    "    extracted = tldextract.extract(url)\n",
    "    subdomain, domain, suffix = extracted\n",
    "    # add both versions of domain.suffix and subdomain.domain.suffix\n",
    "    full = \"\"\n",
    "    # with subdomain\n",
    "    if len(subdomain) > 0 and len(suffix) > 0:\n",
    "        #print(f\"{subdomain}.{domain}.{suffix}\")\n",
    "        full = f\"{subdomain}.{domain}.{suffix}\"\n",
    "        if len(full) > 0:\n",
    "            full = full[4:].strip('/') if full.startswith(\"www.\") else full.strip('/')\n",
    "            if full in gmm_news_sources: # ******* gmm_news_sources_ added here *******\n",
    "                return full\n",
    "            # hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "            # hostnames.add(full.replace(\"www.\",\"\").strip('/'))\n",
    "    # without subdomain\n",
    "    full = f\"{domain}.{suffix}\"\n",
    "    if len(full) > 0 and len(suffix) > 0:\n",
    "        full = full[4:].strip('/') if full.startswith(\"www.\") else full.strip('/')\n",
    "        if full in gmm_news_sources: # ******* gmm_news_sources_ added here *******\n",
    "            return full\n",
    "    return \"\"\n",
    "        # hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "        # hostnames.add(full.replace(\"www.\",\"\").strip('/'))\n",
    "    # return hostnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes.com\n",
      "nytimes.com\n",
      "nytimes.com\n"
     ]
    }
   ],
   "source": [
    "# function try out\n",
    "print(get_hostname(\"https://www.nytimes.com\"))\n",
    "print(get_hostname(\"http://www.aiaia.nytimes.com/add\"))\n",
    "print(get_hostname(\"www.nytimes.com/additional\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"realtor.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zst_files = [\"RS_2021-01.zst\", \"RS_2021-02.zst\", \"RS_2021-03.zst\", \"RS_2021-04.zst\", \"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "# zst_files = [\"RS_2021-05.zst\", \"RS_2021-06.zst\"]\n",
    "zst_filepath = \"E:/thesis_data/\" # D for ThinkPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_srid = dict()\n",
    "\n",
    "posts_with_urls = list()\n",
    "posts_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'baz', 'quux', 'tup_1', 'tup_2', 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "x = [[], ['foo'], ['bar', 'baz'], ['quux'], (\"tup_1\", \"tup_2\"), {1:\"one\", 2:\"two\"}]\n",
    "print(list(itertools.chain(*x)))\n",
    "# print([element for sub in x for element in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2022-02-09 08:10:23.906538\n",
      "***** Start processing for RS_2021-02.zst *****\n",
      "processed 500000 by 08:11:45\n",
      "processed 1000000 by 08:13:07\n",
      "processed 1500000 by 08:14:24\n",
      "processed 2000000 by 08:15:39\n",
      "processed 2500000 by 08:16:51\n",
      "processed 3000000 by 08:18:06\n",
      "processed 3500000 by 08:19:23\n",
      "processed 4000000 by 08:20:36\n",
      "processed 4500000 by 08:22:01\n",
      "processed 5000000 by 08:23:20\n",
      "processed 5500000 by 08:24:39\n",
      "processed 6000000 by 08:25:53\n",
      "processed 6500000 by 08:27:09\n",
      "processed 7000000 by 08:28:33\n",
      "processed 7500000 by 08:29:47\n",
      "processed 8000000 by 08:31:03\n",
      "processed 8500000 by 08:32:16\n",
      "processed 9000000 by 08:33:29\n",
      "processed 9500000 by 08:34:41\n",
      "processed 10000000 by 08:35:52\n",
      "processed 10500000 by 08:37:05\n",
      "processed 11000000 by 08:38:18\n",
      "processed 11500000 by 08:39:30\n",
      "processed 12000000 by 08:40:50\n",
      "processed 12500000 by 08:42:04\n",
      "processed 13000000 by 08:43:19\n",
      "processed 13500000 by 08:44:33\n",
      "processed 14000000 by 08:45:45\n",
      "processed 14500000 by 08:47:01\n",
      "processed 15000000 by 08:48:15\n",
      "processed 15500000 by 08:49:28\n",
      "processed 16000000 by 08:50:39\n",
      "processed 16500000 by 08:51:54\n",
      "processed 17000000 by 08:53:07\n",
      "processed 17500000 by 08:54:21\n",
      "processed 18000000 by 08:55:34\n",
      "processed 18500000 by 08:56:53\n",
      "processed 19000000 by 08:58:13\n",
      "processed 19500000 by 08:59:31\n",
      "processed 20000000 by 09:00:51\n",
      "processed 20500000 by 09:02:11\n",
      "processed 21000000 by 09:03:31\n",
      "processed 21500000 by 09:04:49\n",
      "processed 22000000 by 09:06:07\n",
      "processed 22500000 by 09:07:28\n",
      "processed 23000000 by 09:08:42\n",
      "processed 23500000 by 09:09:57\n",
      "processed 24000000 by 09:11:12\n",
      "processed 24500000 by 09:12:27\n",
      "processed 25000000 by 09:13:42\n",
      "processed 25500000 by 09:14:59\n",
      "processed 26000000 by 09:16:10\n",
      "processed 26500000 by 09:17:26\n",
      "processed 27000000 by 09:18:34\n",
      "processed 27500000 by 09:19:48\n",
      "processed 28000000 by 09:21:03\n",
      "processed 28500000 by 09:22:17\n",
      "processed 29000000 by 09:23:32\n",
      "processed 29500000 by 09:24:46\n",
      "processed 30000000 by 09:26:00\n",
      "processed 30500000 by 09:27:15\n",
      "processed 31000000 by 09:28:28\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-02.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-03.zst *****\n",
      "processed 500000 by 09:30:04\n",
      "processed 1000000 by 09:31:18\n",
      "processed 1500000 by 09:32:33\n",
      "processed 2000000 by 09:33:48\n",
      "processed 2500000 by 09:35:04\n",
      "processed 3000000 by 09:36:20\n",
      "processed 3500000 by 09:37:36\n",
      "processed 4000000 by 09:38:52\n",
      "processed 4500000 by 09:40:06\n",
      "processed 5000000 by 09:41:20\n",
      "processed 5500000 by 09:42:36\n",
      "processed 6000000 by 09:43:50\n",
      "processed 6500000 by 09:45:04\n",
      "processed 7000000 by 09:46:18\n",
      "processed 7500000 by 09:47:35\n",
      "processed 8000000 by 09:48:49\n",
      "processed 8500000 by 09:50:05\n",
      "processed 9000000 by 09:51:18\n",
      "processed 9500000 by 09:52:35\n",
      "processed 10000000 by 09:53:49\n",
      "processed 10500000 by 09:55:03\n",
      "processed 11000000 by 09:56:14\n",
      "processed 11500000 by 09:57:30\n",
      "processed 12000000 by 09:58:43\n",
      "processed 12500000 by 09:59:56\n",
      "processed 13000000 by 10:01:12\n",
      "processed 13500000 by 10:02:26\n",
      "processed 14000000 by 10:03:42\n",
      "processed 14500000 by 10:04:55\n",
      "processed 15000000 by 10:06:10\n",
      "processed 15500000 by 10:07:26\n",
      "processed 16000000 by 10:08:42\n",
      "processed 16500000 by 10:09:57\n",
      "processed 17000000 by 10:11:17\n",
      "processed 17500000 by 10:12:42\n",
      "processed 18000000 by 10:14:08\n",
      "processed 18500000 by 10:15:46\n",
      "processed 19000000 by 10:17:22\n",
      "processed 19500000 by 10:19:09\n",
      "processed 20000000 by 10:20:57\n",
      "processed 20500000 by 10:22:32\n",
      "processed 21000000 by 10:24:08\n",
      "processed 21500000 by 10:25:45\n",
      "processed 22000000 by 10:27:10\n",
      "processed 22500000 by 10:28:40\n",
      "processed 23000000 by 10:29:56\n",
      "processed 23500000 by 10:31:14\n",
      "processed 24000000 by 10:32:30\n",
      "processed 24500000 by 10:33:48\n",
      "processed 25000000 by 10:35:03\n",
      "processed 25500000 by 10:36:22\n",
      "processed 26000000 by 10:37:51\n",
      "processed 26500000 by 10:39:20\n",
      "processed 27000000 by 10:40:50\n",
      "processed 27500000 by 10:42:18\n",
      "processed 28000000 by 10:43:43\n",
      "processed 28500000 by 10:45:08\n",
      "processed 29000000 by 10:46:33\n",
      "processed 29500000 by 10:47:56\n",
      "processed 30000000 by 10:49:21\n",
      "processed 30500000 by 10:50:43\n",
      "processed 31000000 by 10:52:06\n",
      "processed 31500000 by 10:53:31\n",
      "processed 32000000 by 10:54:56\n",
      "processed 32500000 by 10:56:19\n",
      "processed 33000000 by 10:57:48\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-03.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-04.zst *****\n",
      "processed 500000 by 10:59:15\n",
      "processed 1000000 by 11:00:35\n",
      "processed 1500000 by 11:01:55\n",
      "processed 2000000 by 11:03:18\n",
      "processed 2500000 by 11:04:39\n",
      "processed 3000000 by 11:06:02\n",
      "processed 3500000 by 11:07:26\n",
      "processed 4000000 by 11:08:46\n",
      "processed 4500000 by 11:10:06\n",
      "processed 5000000 by 11:11:31\n",
      "processed 5500000 by 11:12:54\n",
      "processed 6000000 by 11:14:15\n",
      "processed 6500000 by 11:15:35\n",
      "processed 7000000 by 11:17:01\n",
      "processed 7500000 by 11:18:23\n",
      "processed 8000000 by 11:19:46\n",
      "processed 8500000 by 11:21:06\n",
      "processed 9000000 by 11:22:27\n",
      "processed 9500000 by 11:23:47\n",
      "processed 10000000 by 11:25:08\n",
      "processed 10500000 by 11:26:27\n",
      "processed 11000000 by 11:27:51\n",
      "processed 11500000 by 11:29:13\n",
      "processed 12000000 by 11:30:36\n",
      "processed 12500000 by 11:31:57\n",
      "processed 13000000 by 11:33:19\n",
      "processed 13500000 by 11:34:43\n",
      "processed 14000000 by 11:36:09\n",
      "processed 14500000 by 11:37:32\n",
      "processed 15000000 by 11:39:01\n",
      "processed 15500000 by 11:40:25\n",
      "processed 16000000 by 11:41:47\n",
      "processed 16500000 by 11:43:07\n",
      "processed 17000000 by 11:44:26\n",
      "processed 17500000 by 11:45:52\n",
      "processed 18000000 by 11:47:17\n",
      "processed 18500000 by 11:48:39\n",
      "processed 19000000 by 11:50:02\n",
      "processed 19500000 by 11:51:26\n",
      "processed 20000000 by 11:52:50\n",
      "processed 20500000 by 11:54:15\n",
      "processed 21000000 by 11:55:40\n",
      "processed 21500000 by 11:57:11\n",
      "processed 22000000 by 11:58:35\n",
      "processed 22500000 by 11:59:58\n",
      "processed 23000000 by 12:01:22\n",
      "processed 23500000 by 12:02:46\n",
      "processed 24000000 by 12:04:10\n",
      "processed 24500000 by 12:05:31\n",
      "processed 25000000 by 12:06:55\n",
      "processed 25500000 by 12:08:18\n",
      "processed 26000000 by 12:09:42\n",
      "processed 26500000 by 12:11:04\n",
      "processed 27000000 by 12:12:24\n",
      "processed 27500000 by 12:13:47\n",
      "processed 28000000 by 12:15:09\n",
      "processed 28500000 by 12:16:29\n",
      "processed 29000000 by 12:17:54\n",
      "processed 29500000 by 12:19:14\n",
      "processed 30000000 by 12:20:31\n",
      "processed 30500000 by 12:21:48\n",
      "processed 31000000 by 12:23:04\n",
      "processed 31500000 by 12:24:23\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-04.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-05.zst *****\n",
      "processed 500000 by 12:25:58\n",
      "processed 1000000 by 12:27:13\n",
      "processed 1500000 by 12:28:27\n",
      "processed 2000000 by 12:29:42\n",
      "processed 2500000 by 12:30:55\n",
      "processed 3000000 by 12:32:10\n",
      "processed 3500000 by 12:33:23\n",
      "processed 4000000 by 12:34:39\n",
      "processed 4500000 by 12:35:53\n",
      "processed 5000000 by 12:37:08\n",
      "processed 5500000 by 12:38:25\n",
      "processed 6000000 by 12:39:38\n",
      "processed 6500000 by 12:40:54\n",
      "processed 7000000 by 12:42:08\n",
      "processed 7500000 by 12:43:23\n",
      "processed 8000000 by 12:44:38\n",
      "processed 8500000 by 12:45:51\n",
      "processed 9000000 by 12:47:06\n",
      "processed 9500000 by 12:48:19\n",
      "processed 10000000 by 12:49:34\n",
      "processed 10500000 by 12:50:47\n",
      "processed 11000000 by 12:52:03\n",
      "processed 11500000 by 12:53:18\n",
      "processed 12000000 by 12:54:32\n",
      "processed 12500000 by 12:55:48\n",
      "processed 13000000 by 12:57:03\n",
      "processed 13500000 by 12:58:18\n",
      "processed 14000000 by 12:59:32\n",
      "processed 14500000 by 13:00:46\n",
      "processed 15000000 by 13:02:01\n",
      "processed 15500000 by 13:03:15\n",
      "processed 16000000 by 13:04:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 16500000 by 13:05:50\n",
      "processed 17000000 by 13:07:04\n",
      "processed 17500000 by 13:08:18\n",
      "processed 18000000 by 13:09:32\n",
      "processed 18500000 by 13:10:46\n",
      "processed 19000000 by 13:11:56\n",
      "processed 19500000 by 13:13:09\n",
      "processed 20000000 by 13:14:20\n",
      "processed 20500000 by 13:15:32\n",
      "processed 21000000 by 13:16:45\n",
      "processed 21500000 by 13:17:58\n",
      "processed 22000000 by 13:19:11\n",
      "processed 22500000 by 13:20:24\n",
      "processed 23000000 by 13:21:36\n",
      "processed 23500000 by 13:22:51\n",
      "processed 24000000 by 13:24:04\n",
      "processed 24500000 by 13:25:18\n",
      "processed 25000000 by 13:26:31\n",
      "processed 25500000 by 13:27:44\n",
      "processed 26000000 by 13:28:58\n",
      "processed 26500000 by 13:30:10\n",
      "processed 27000000 by 13:31:31\n",
      "processed 27500000 by 13:32:51\n",
      "processed 28000000 by 13:34:13\n",
      "processed 28500000 by 13:35:36\n",
      "processed 29000000 by 13:37:00\n",
      "processed 29500000 by 13:38:25\n",
      "processed 30000000 by 13:39:48\n",
      "processed 30500000 by 13:41:16\n",
      "processed 31000000 by 13:42:42\n",
      "processed 31500000 by 13:44:02\n",
      "processed 32000000 by 13:45:26\n",
      "processed 32500000 by 13:46:50\n",
      "processed 33000000 by 13:48:11\n",
      "processed 33500000 by 13:49:31\n",
      "processed 34000000 by 13:50:55\n",
      "processed 34500000 by 13:52:19\n",
      "processed 35000000 by 13:53:45\n",
      "processed 35500000 by 13:55:09\n",
      "processed 36000000 by 13:56:32\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-05.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-06.zst *****\n",
      "processed 500000 by 13:58:49\n",
      "processed 1000000 by 14:00:12\n",
      "processed 1500000 by 14:01:35\n",
      "processed 2000000 by 14:02:54\n",
      "processed 2500000 by 14:04:14\n",
      "processed 3000000 by 14:05:38\n",
      "processed 3500000 by 14:07:05\n",
      "processed 4000000 by 14:08:29\n",
      "processed 4500000 by 14:10:02\n",
      "processed 5000000 by 14:11:34\n",
      "processed 5500000 by 14:13:05\n",
      "processed 6000000 by 14:14:29\n",
      "processed 6500000 by 14:15:47\n",
      "processed 7000000 by 14:17:28\n",
      "processed 7500000 by 14:19:07\n",
      "processed 8000000 by 14:21:16\n",
      "processed 8500000 by 14:23:25\n",
      "processed 9000000 by 14:25:31\n",
      "processed 9500000 by 14:27:35\n",
      "processed 10000000 by 14:29:33\n",
      "processed 10500000 by 14:31:18\n",
      "processed 11000000 by 14:33:01\n",
      "processed 11500000 by 14:34:48\n",
      "processed 12000000 by 14:36:39\n",
      "processed 12500000 by 14:38:40\n",
      "processed 13000000 by 14:40:50\n",
      "processed 13500000 by 14:42:56\n",
      "processed 14000000 by 14:44:53\n",
      "processed 14500000 by 14:46:37\n",
      "processed 15000000 by 14:48:23\n",
      "processed 15500000 by 14:50:10\n",
      "processed 16000000 by 14:51:56\n",
      "processed 16500000 by 14:53:49\n",
      "processed 17000000 by 14:55:40\n",
      "processed 17500000 by 14:57:40\n",
      "processed 18000000 by 14:59:23\n",
      "processed 18500000 by 15:01:11\n",
      "processed 19000000 by 15:02:49\n",
      "processed 19500000 by 15:04:11\n",
      "processed 20000000 by 15:05:39\n",
      "processed 20500000 by 15:07:06\n",
      "processed 21000000 by 15:08:35\n",
      "processed 21500000 by 15:09:50\n",
      "processed 22000000 by 15:11:13\n",
      "processed 22500000 by 15:12:38\n",
      "processed 23000000 by 15:14:02\n",
      "processed 23500000 by 15:15:27\n",
      "processed 24000000 by 15:16:51\n",
      "processed 24500000 by 15:18:22\n",
      "processed 25000000 by 15:19:50\n",
      "processed 25500000 by 15:21:19\n",
      "processed 26000000 by 15:22:47\n",
      "processed 26500000 by 15:24:16\n",
      "processed 27000000 by 15:25:43\n",
      "processed 27500000 by 15:27:13\n",
      "processed 28000000 by 15:28:42\n",
      "processed 28500000 by 15:30:17\n",
      "processed 29000000 by 15:31:52\n",
      "processed 29500000 by 15:33:22\n",
      "processed 30000000 by 15:34:57\n",
      "processed 30500000 by 15:36:29\n",
      "processed 31000000 by 15:37:59\n",
      "processed 31500000 by 15:39:29\n",
      "processed 32000000 by 15:41:04\n",
      "processed 32500000 by 15:42:35\n",
      "processed 33000000 by 15:44:05\n",
      "processed 33500000 by 15:45:35\n",
      "processed 34000000 by 15:47:08\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-06.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "finish time: 2022-02-09 15:47:35.003050\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "for zst_file in zst_files[1:]:\n",
    "    ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    subreddit_ns = defaultdict(Counter)\n",
    "    print(\"***** Start processing for {} *****\".format(zst_file))\n",
    "    with open(zst_filepath+zst_file, 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "        with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            url_of_our_ns = False \n",
    "            # ^this is to keep track if this post has ns url that we care about.\n",
    "            # if so, add to posts_with_urls once.\n",
    "            for d in text_stream:\n",
    "                line = json.loads(d)\n",
    "                subreddit, subreddit_id = line['subreddit'], line['subreddit_id']\n",
    "                if subreddit not in subreddit_srid:\n",
    "                    subreddit_srid[subreddit] = subreddit_id\n",
    "                URLs = findURLs(line['url']) + findURLs(line['selftext'])\n",
    "                hostnames = [get_hostname(url) for url in URLs]\n",
    "                URLs = [element for sub in hostnames for element in sub]\n",
    "                # URLs = itertools.chain(*hostnames)\n",
    "                # print(\"URLs:\", URLs)\n",
    "                # if len(URLs) > 10: print(line['selftext'])\n",
    "                for url in URLs:\n",
    "                    if url in gmm_news_sources: # instead of the full  news_sources\n",
    "                        if not url_of_our_ns:\n",
    "                            # posts_with_urls.append(line)\n",
    "                            url_of_our_ns = True\n",
    "                        ns_subreddit[url][subreddit] += 1\n",
    "                        subreddit_ns[subreddit][url] += 1\n",
    "                        # break\n",
    "                        # print(f\"ns_subreddit: {ns_subreddit}\")\n",
    "                url_of_our_ns = False\n",
    "                counter += 1\n",
    "                if counter%500000 == 0: \n",
    "                    print(\"processed {} by {}\".format(counter, str(datetime.datetime.now())[11:19]))\n",
    "                \n",
    "    \n",
    "    print(\"-------------------------------- Done reading, will write files now --------------------------------\")\n",
    "    \n",
    "    # write into files separated by months\n",
    "    with open(\"ns_subreddit_{}.json\".format(zst_file[3:10]), \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(ns_subreddit, outfile, indent=4)\n",
    "        \n",
    "    with open(\"subreddit_ns_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile1:\n",
    "        json.dump(subreddit_ns, outfile1, indent=4)\n",
    "        \n",
    "    with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "        json.dump(subreddit_srid, infile_srid, indent=4)\n",
    "        \n",
    "    # with open(\"E:\\\\thesis_data\\\\posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "        # json.dump(posts_with_urls, outfile2, indent=4)\n",
    "        \n",
    "    # ns_subreddit = defaultdict(Counter) # counting how many time a news source appears in each subreddit\n",
    "    # subreddit_ns = defaultdict(Counter)\n",
    "    # subreddit_srid = dict()\n",
    "    posts_with_urls = list()\n",
    "    counter = 0\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------- Done processing for {} --------------------------------\".format(zst_file))\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"tunein.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "#     json.dump(subreddit_srid, infile_srid)\n",
    "        \n",
    "# with open(\"D:/thesis_data/posts_with_urls_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile2:\n",
    "#     json.dump(posts_with_urls, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-03'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zst_file[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of posts read from \n",
    "1. January 2021:\n",
    "2. February 2021: 31,161,912\n",
    "3. March 2021: 33,0061,03\n",
    "4. April 2021:\n",
    "5. May 2021: >36M\n",
    "6. June 2021: >34M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"subreddit_srid_{}.json\".format(\"2021-04\"), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "    json.dump(subreddit_srid, infile_srid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588719"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_with_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3686"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ns_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3686 news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddit_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20755 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811504"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_with_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ns_subreddit.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(ns_subreddit, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subreddit_ns.json\", \"w\", encoding = \"utf-8\") as outfile1:\n",
    "    json.dump(subreddit_ns, outfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"posts_with_urls\", \"w\", encoding = \"utf-8\") as outfile2:\n",
    "    json.dump(posts_with_urls, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31616206"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"counter_april21.json\", \"w\", encoding = \"utf-8\") as counterfile:\n",
    "    json.dump(counter, counterfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting subreddit names and ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_id = defaultdict(set)\n",
    "id_subreddit = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2021-11-05 01:59:23.609969\n",
      "at 500000\n",
      "at 1000000\n",
      "at 1500000\n",
      "at 2000000\n",
      "at 2500000\n",
      "at 3000000\n",
      "at 3500000\n",
      "at 4000000\n",
      "at 4500000\n",
      "at 5000000\n",
      "at 5500000\n",
      "at 6000000\n",
      "at 6500000\n",
      "at 7000000\n",
      "at 7500000\n",
      "at 8000000\n",
      "at 8500000\n",
      "at 9000000\n",
      "at 9500000\n",
      "at 10000000\n",
      "at 10500000\n",
      "at 11000000\n",
      "at 11500000\n",
      "at 12000000\n",
      "at 12500000\n",
      "at 13000000\n",
      "at 13500000\n",
      "at 14000000\n",
      "at 14500000\n",
      "at 15000000\n",
      "at 15500000\n",
      "at 16000000\n",
      "at 16500000\n",
      "at 17000000\n",
      "at 17500000\n",
      "at 18000000\n",
      "at 18500000\n",
      "at 19000000\n",
      "at 19500000\n",
      "at 20000000\n",
      "at 20500000\n",
      "at 21000000\n",
      "at 21500000\n",
      "at 22000000\n",
      "at 22500000\n",
      "at 23000000\n",
      "at 23500000\n",
      "at 24000000\n",
      "at 24500000\n",
      "at 25000000\n",
      "at 25500000\n",
      "at 26000000\n",
      "at 26500000\n",
      "at 27000000\n",
      "at 27500000\n",
      "at 28000000\n",
      "at 28500000\n",
      "at 29000000\n",
      "at 29500000\n",
      "at 30000000\n",
      "at 30500000\n",
      "at 31000000\n",
      "at 31500000\n",
      "finish time: 2021-11-05 03:35:19.988106\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "with open(\"D://Wellesley/F21/thesis_zst_data/RS_2021-04.zst\", 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "    with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "        text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "        for d in text_stream:\n",
    "            line = json.loads(d)\n",
    "            sr, sr_id = line['subreddit'], line['subreddit_id']\n",
    "            subreddit_id[sr].add(sr_id)\n",
    "            id_subreddit[sr_id].add(sr)\n",
    "#             URLs = findURLs(line['url']) + findURLs(line['selftext'])\n",
    "#             URLs = [get_hostname(url) for url in URLs]\n",
    "#             # print(\"URLs:\", URLs)\n",
    "#             # if len(URLs) > 10: print(line['selftext'])\n",
    "#             for url in URLs:\n",
    "#                 if url in news_sources:\n",
    "#                     posts_with_urls.append(line)\n",
    "#                     ns_subreddit[url][subreddit] += 1\n",
    "#                     subreddit_ns[subreddit][url] += 1\n",
    "            counter += 1\n",
    "            if counter%500000 == 0: print(f\"at {counter}\")\n",
    "                \n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639811"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639811"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddit_id:\n",
    "    if len(subreddit_id[s]) != 1:\n",
    "        print(f\"{s} is invalid, length {len(subreddit_id[s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in id_subreddit:\n",
    "    if len(id_subreddit[s]) != 1:\n",
    "        print(f\"{s} is invalid, length {len(id_subreddit[s])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make all values to be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddit_id:\n",
    "    subreddit_id[s] = list(subreddit_id[s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in id_subreddit:\n",
    "    id_subreddit[i] = list(id_subreddit[i])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the same number of `id`s and `subreddit`s. Good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subreddit_id.json\", \"w\", encoding = \"utf-8\") as outfile_si:\n",
    "    json.dump(subreddit_id, outfile_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"id_subreddit.json\", \"w\", encoding = \"utf-8\") as outfile_is:\n",
    "    json.dump(id_subreddit, outfile_is)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

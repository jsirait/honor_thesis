{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary look at subreddits for 6k news sources\n",
    "\n",
    "This notebook is to extract how many times each news source is mentioned in each subreddits.\n",
    "\n",
    "Final data structures:\n",
    "\n",
    "Six files of `ns_subreddit_{month}.json` for month January, February, March, April, May, and June. Each of the file is a dictionary, with keys being news sources. The values would be a triple (`mention_count`, `mention_count_weighted_by_upvote_ratio`, `mention_count_weighted_by_number_of_comments`). For example, if in January `nytimes.com` is only mentioned in subreddit `news` two times with the following details:\n",
    "* first mention has 0.60 upvote ratio and 3 comments\n",
    "* second mention has 1 upvote ratio and 2 comments\n",
    "\n",
    "Then `ns_subreddit_2021-01.json` will have a dictionary item of `{nytimes.com: (2, 1.60, 5)}`. This is because `nytimes.com` is mentioned in total two times. The `mention_count_weighted_by_upvote_ratio` is `1*0.60 + 1*1`. The `mention_count_weighted_by_number_of_comments` is `1*3 + 1*2`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zstandard in c:\\users\\user200803\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.15.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User200803\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import io\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import datetime, time\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-16 20:00:01.883972\n",
      "20:00:01\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print(str(datetime.datetime.now())[11:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading news sources that are the intersection of GDELT and Muck Rack news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gmm_intersection.json\", \"r\") as infile:\n",
    "    news_sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wcyb.com',\n",
       " 'betootaadvocate.com',\n",
       " 'wikinews.org',\n",
       " 'oklahoman.com',\n",
       " 'codepink.org',\n",
       " 'veteranstoday.com',\n",
       " 'arktimes.com',\n",
       " 'thelancet.com',\n",
       " 'jewishworldreview.com',\n",
       " 'mcsweeneys.net']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sources[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open reddit data from April 2021 for exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://arxiv.org/pdf/2001.08435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an entry of data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'all_awardings': [], \n",
    "    'allow_live_comments': False, \n",
    "    'archived': False, \n",
    "    'author': 'elanglohablante9805', \n",
    "    'author_created_utc': 1609519842, \n",
    "    'author_flair_background_color': '#ffb000', \n",
    "    'author_flair_css_class': None, \n",
    "    'author_flair_richtext': [], \n",
    "    'author_flair_template_id': '4f908eaa-9664-11ea-a567-0ed46a42aec3', \n",
    "    'author_flair_text': 'Historiador 游닆 | 80-Day Streak 游댠', \n",
    "    'author_flair_text_color': 'dark', \n",
    "    'author_flair_type': 'text', \n",
    "    'author_fullname': 't2_9lr431i4', \n",
    "    'author_patreon_flair': False, \n",
    "    'author_premium': False, \n",
    "    'can_gild': True, \n",
    "    'category': None, \n",
    "    'content_categories': None, \n",
    "    'contest_mode': False, \n",
    "    'created_utc': 1617235201, \n",
    "    'discussion_type': None, \n",
    "    'distinguished': None, \n",
    "    'domain': 'self.WriteStreakES', \n",
    "    'edited': False, \n",
    "    'gilded': 0, \n",
    "    'gildings': {}, \n",
    "    'hidden': False, \n",
    "    'hide_score': False, \n",
    "    'id': 'mhj2hj', \n",
    "    'is_created_from_ads_ui': False, \n",
    "    'is_crosspostable': True, \n",
    "    'is_meta': False, \n",
    "    'is_original_content': False, \n",
    "    'is_reddit_media_domain': False, \n",
    "    'is_robot_indexable': True, \n",
    "    'is_self': True, \n",
    "    'is_video': False, \n",
    "    'link_flair_background_color': '', \n",
    "    'link_flair_css_class': None, \n",
    "    'link_flair_richtext': [], \n",
    "    'link_flair_text': None, \n",
    "    'link_flair_text_color': 'dark', \n",
    "    'link_flair_type': 'text', \n",
    "    'locked': False,\n",
    "    'media': None, \n",
    "    'media_embed': {}, \n",
    "    'media_only': False, \n",
    "    'name': 't3_mhj2hj', \n",
    "    'no_follow': True, \n",
    "    'num_comments': 2, \n",
    "    'num_crossposts': 0, \n",
    "    'over_18': False, \n",
    "    'parent_whitelist_status': None, \n",
    "    'permalink': '/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'pinned': False, \n",
    "    'pwls': None, \n",
    "    'quarantine': False, \n",
    "    'removed_by_category': None, \n",
    "    'retrieved_utc': 1623447663, \n",
    "    'score': 1, \n",
    "    'secure_media': None, \n",
    "    'secure_media_embed': {}, \n",
    "    'selftext': 'Los p치jaros est치n cantando, las hierbas verdes est치n brotando, y tengo alergias.  Esto es la temporada de las alergias.  Estornudo cada ma침ana cuando me despierto, y otra vez si voy afuera.  Necesito tomar medicina cada d칤a, pero no funciona tan bien. \\n\\nPor fuera, las lomas son bonitas porque son verdes y los robles tienen hojas nuevas.  Por el fin de semana,  hago caminatas pero cuando regreso a casa, necesito ducharme para remover el polen.\\n\\nCuando me jubile, voy a viajar al desierto cada a침o por toda la primavera.  No me gustar칤a quedarme aqu칤.', \n",
    "    'send_replies': True, \n",
    "    'spoiler': False, \n",
    "    'stickied': False, \n",
    "    'subreddit': 'WriteStreakES', \n",
    "    'subreddit_id': 't5_2eamt5', \n",
    "    'subreddit_subscribers': 2205, \n",
    "    'subreddit_type': 'public', \n",
    "    'suggested_sort': None, \n",
    "    'thumbnail': 'self', \n",
    "    'thumbnail_height': None, \n",
    "    'thumbnail_width': None, \n",
    "    'title': 'Streak 90: Ha llegado la primavera', \n",
    "    'top_awarded_type': None, \n",
    "    'total_awards_received': 0, \n",
    "    'treatment_tags': [], \n",
    "    'upvote_ratio': 1.0, \n",
    "    'url': 'https://www.reddit.com/r/WriteStreakES/comments/mhj2hj/streak_90_ha_llegado_la_primavera/', \n",
    "    'whitelist_status': None, 'wls': None}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting count info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctx = zstd.ZstdDecompressor(max_window_size=2147483648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findURLs(phrase):\n",
    "    regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)')\n",
    "    url = re.findall(regex, phrase)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lol.com',\n",
       " 'https://nytimes.com/2021/10/19/us/politics/trump-border.html']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out\n",
    "findURLs(\"does this find https://lol.com or nytimes.com/2021/10/19/us/politics/trump-border.html or https://nytimes.com/2021/10/19/us/politics/trump-border.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname(url, uri_type='both'):\n",
    "    \"\"\"Get the host name from the url\"\"\"\n",
    "    hostnames = set()\n",
    "    extracted = tldextract.extract(url)\n",
    "    subdomain, domain, suffix = extracted\n",
    "    # add both versions of domain.suffix and subdomain.domain.suffix\n",
    "    full = \"\"\n",
    "    # with subdomain\n",
    "    if len(subdomain) > 0 and len(suffix) > 0:\n",
    "        #print(f\"{subdomain}.{domain}.{suffix}\")\n",
    "        full = f\"{subdomain}.{domain}.{suffix}\"\n",
    "        if len(full) > 0:\n",
    "            hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "    full = f\"{domain}.{suffix}\"\n",
    "    if len(full) > 0 and len(suffix) > 0:\n",
    "        hostnames.add(full[4:].strip('/')) if full.startswith(\"www.\") else hostnames.add(full.strip('/'))\n",
    "    return hostnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs.wellesley.edu', 'wellesley.edu'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hostname(\"https://cs.wellesley.edu/~cs313/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nytimes.com'}\n",
      "{'nytimes.com', 'aiaia.nytimes.com'}\n",
      "{'nytimes.com'}\n"
     ]
    }
   ],
   "source": [
    "# function try out\n",
    "print(get_hostname(\"https://www.nytimes.com\"))\n",
    "print(get_hostname(\"http://www.aiaia.nytimes.com/add\"))\n",
    "print(get_hostname(\"www.nytimes.com/additional\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"realtor.com\" in news_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "zst_files = [\"RS_2021-01.zst\", \"RS_2021-02.zst\", \"RS_2021-03.zst\", \"RS_2021-04.zst\", \"RS_2021-05.zst\", \"RS_2021-06.zst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_srid = dict() # to keep track of subreddit id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'baz', 'quux', 'tup_1', 'tup_2', 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# example of flattening\n",
    "import itertools\n",
    "x = [[], ['foo'], ['bar', 'baz'], ['quux'], (\"tup_1\", \"tup_2\"), {1:\"one\", 2:\"two\"}]\n",
    "print(list(itertools.chain(*x)))\n",
    "# print([element for sub in x for element in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2022-03-16 20:00:02.403276\n",
      "***** Start processing for RS_2021-01.zst *****\n",
      "processed 500000 by 20:02:16\n",
      "processed 1000000 by 20:04:32\n",
      "processed 1500000 by 20:06:47\n",
      "processed 2000000 by 20:08:59\n",
      "processed 2500000 by 20:11:10\n",
      "processed 3000000 by 20:13:22\n",
      "processed 3500000 by 20:15:34\n",
      "processed 4000000 by 20:17:49\n",
      "processed 4500000 by 20:20:02\n",
      "processed 5000000 by 20:22:16\n",
      "processed 5500000 by 20:24:30\n",
      "processed 6000000 by 20:26:45\n",
      "processed 6500000 by 20:28:59\n",
      "processed 7000000 by 20:31:12\n",
      "processed 7500000 by 20:33:23\n",
      "processed 8000000 by 20:35:38\n",
      "processed 8500000 by 20:37:55\n",
      "processed 9000000 by 20:40:10\n",
      "processed 9500000 by 20:42:24\n",
      "processed 10000000 by 20:44:40\n",
      "processed 10500000 by 20:46:59\n",
      "processed 11000000 by 20:49:22\n",
      "processed 11500000 by 20:51:38\n",
      "processed 12000000 by 20:53:58\n",
      "processed 12500000 by 20:56:13\n",
      "processed 13000000 by 20:58:26\n",
      "processed 13500000 by 21:00:39\n",
      "processed 14000000 by 21:03:01\n",
      "processed 14500000 by 21:05:17\n",
      "processed 15000000 by 21:07:32\n",
      "processed 15500000 by 21:09:46\n",
      "processed 16000000 by 21:11:58\n",
      "processed 16500000 by 21:14:11\n",
      "processed 17000000 by 21:16:27\n",
      "processed 17500000 by 21:18:45\n",
      "processed 18000000 by 21:20:59\n",
      "processed 18500000 by 21:23:12\n",
      "processed 19000000 by 21:25:26\n",
      "processed 19500000 by 21:27:39\n",
      "processed 20000000 by 21:29:53\n",
      "processed 20500000 by 21:32:05\n",
      "processed 21000000 by 21:34:16\n",
      "processed 21500000 by 21:36:32\n",
      "processed 22000000 by 21:38:51\n",
      "processed 22500000 by 21:41:12\n",
      "processed 23000000 by 21:43:29\n",
      "processed 23500000 by 21:45:47\n",
      "processed 24000000 by 21:48:03\n",
      "processed 24500000 by 21:50:27\n",
      "processed 25000000 by 21:52:53\n",
      "processed 25500000 by 21:55:16\n",
      "processed 26000000 by 21:57:34\n",
      "processed 26500000 by 21:59:53\n",
      "processed 27000000 by 22:02:15\n",
      "processed 27500000 by 22:04:27\n",
      "processed 28000000 by 22:06:35\n",
      "processed 28500000 by 22:08:41\n",
      "processed 29000000 by 22:10:46\n",
      "processed 29500000 by 22:12:54\n",
      "processed 30000000 by 22:15:05\n",
      "processed 30500000 by 22:17:09\n",
      "processed 31000000 by 22:19:18\n",
      "processed 31500000 by 22:21:47\n",
      "processed 32000000 by 22:24:21\n",
      "processed 32500000 by 22:26:43\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-01.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-02.zst *****\n",
      "processed 500000 by 22:30:06\n",
      "processed 1000000 by 22:32:34\n",
      "processed 1500000 by 22:34:52\n",
      "processed 2000000 by 22:37:11\n",
      "processed 2500000 by 22:39:29\n",
      "processed 3000000 by 22:41:54\n",
      "processed 3500000 by 22:44:11\n",
      "processed 4000000 by 22:46:26\n",
      "processed 4500000 by 22:48:44\n",
      "processed 5000000 by 22:51:02\n",
      "processed 5500000 by 22:53:28\n",
      "processed 6000000 by 22:55:45\n",
      "processed 6500000 by 22:57:55\n",
      "processed 7000000 by 23:00:13\n",
      "processed 7500000 by 23:02:32\n",
      "processed 8000000 by 23:04:47\n",
      "processed 8500000 by 23:07:03\n",
      "processed 9000000 by 23:09:16\n",
      "processed 9500000 by 23:11:27\n",
      "processed 10000000 by 23:13:43\n",
      "processed 10500000 by 23:15:57\n",
      "processed 11000000 by 23:18:12\n",
      "processed 11500000 by 23:20:29\n",
      "processed 12000000 by 23:22:45\n",
      "processed 12500000 by 23:25:00\n",
      "processed 13000000 by 23:27:16\n",
      "processed 13500000 by 23:29:30\n",
      "processed 14000000 by 23:31:44\n",
      "processed 14500000 by 23:34:06\n",
      "processed 15000000 by 23:36:22\n",
      "processed 15500000 by 23:38:42\n",
      "processed 16000000 by 23:40:55\n",
      "processed 16500000 by 23:43:16\n",
      "processed 17000000 by 23:45:34\n",
      "processed 17500000 by 23:47:52\n",
      "processed 18000000 by 23:50:11\n",
      "processed 18500000 by 23:52:37\n",
      "processed 19000000 by 23:54:52\n",
      "processed 19500000 by 23:57:08\n",
      "processed 20000000 by 23:59:21\n",
      "processed 20500000 by 00:01:38\n",
      "processed 21000000 by 00:04:01\n",
      "processed 21500000 by 00:06:14\n",
      "processed 22000000 by 00:08:31\n",
      "processed 22500000 by 00:10:44\n",
      "processed 23000000 by 00:13:12\n",
      "processed 23500000 by 00:15:26\n",
      "processed 24000000 by 00:17:38\n",
      "processed 24500000 by 00:19:55\n",
      "processed 25000000 by 00:22:16\n",
      "processed 25500000 by 00:24:33\n",
      "processed 26000000 by 00:26:42\n",
      "processed 26500000 by 00:28:59\n",
      "processed 27000000 by 00:31:03\n",
      "processed 27500000 by 00:33:27\n",
      "processed 28000000 by 00:35:50\n",
      "processed 28500000 by 00:38:07\n",
      "processed 29000000 by 00:40:25\n",
      "processed 29500000 by 00:42:44\n",
      "processed 30000000 by 00:45:05\n",
      "processed 30500000 by 00:47:20\n",
      "processed 31000000 by 00:49:36\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-02.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-03.zst *****\n",
      "processed 500000 by 00:52:55\n",
      "processed 1000000 by 00:55:16\n",
      "processed 1500000 by 00:57:36\n",
      "processed 2000000 by 00:59:57\n",
      "processed 2500000 by 01:02:17\n",
      "processed 3000000 by 01:04:35\n",
      "processed 3500000 by 01:06:58\n",
      "processed 4000000 by 01:09:17\n",
      "processed 4500000 by 01:11:31\n",
      "processed 5000000 by 01:14:01\n",
      "processed 5500000 by 01:16:23\n",
      "processed 6000000 by 01:18:40\n",
      "processed 6500000 by 01:21:00\n",
      "processed 7000000 by 01:23:22\n",
      "processed 7500000 by 01:25:43\n",
      "processed 8000000 by 01:28:08\n",
      "processed 8500000 by 01:30:26\n",
      "processed 9000000 by 01:32:52\n",
      "processed 9500000 by 01:35:09\n",
      "processed 10000000 by 01:37:33\n",
      "processed 10500000 by 01:39:48\n",
      "processed 11000000 by 01:42:03\n",
      "processed 11500000 by 01:44:21\n",
      "processed 12000000 by 01:46:35\n",
      "processed 12500000 by 01:48:50\n",
      "processed 13000000 by 01:51:08\n",
      "processed 13500000 by 01:53:33\n",
      "processed 14000000 by 01:55:50\n",
      "processed 14500000 by 01:58:05\n",
      "processed 15000000 by 02:00:25\n",
      "processed 15500000 by 02:02:37\n",
      "processed 16000000 by 02:05:03\n",
      "processed 16500000 by 02:07:19\n",
      "processed 17000000 by 02:09:36\n",
      "processed 17500000 by 02:11:56\n",
      "processed 18000000 by 02:14:25\n",
      "processed 18500000 by 02:16:38\n",
      "processed 19000000 by 02:18:51\n",
      "processed 19500000 by 02:21:11\n",
      "processed 20000000 by 02:23:31\n",
      "processed 20500000 by 02:25:48\n",
      "processed 21000000 by 02:28:03\n",
      "processed 21500000 by 02:30:21\n",
      "processed 22000000 by 02:32:43\n",
      "processed 22500000 by 02:35:00\n",
      "processed 23000000 by 02:37:22\n",
      "processed 23500000 by 02:39:40\n",
      "processed 24000000 by 02:41:59\n",
      "processed 24500000 by 02:44:20\n",
      "processed 25000000 by 02:46:36\n",
      "processed 25500000 by 02:48:54\n",
      "processed 26000000 by 02:51:12\n",
      "processed 26500000 by 02:53:42\n",
      "processed 27000000 by 02:56:11\n",
      "processed 27500000 by 02:58:17\n",
      "processed 28000000 by 03:00:20\n",
      "processed 28500000 by 03:02:22\n",
      "processed 29000000 by 03:04:25\n",
      "processed 29500000 by 03:06:39\n",
      "processed 30000000 by 03:09:00\n",
      "processed 30500000 by 03:11:17\n",
      "processed 31000000 by 03:13:45\n",
      "processed 31500000 by 03:16:01\n",
      "processed 32000000 by 03:18:18\n",
      "processed 32500000 by 03:20:40\n",
      "processed 33000000 by 03:23:00\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-03.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-04.zst *****\n",
      "processed 500000 by 03:25:26\n",
      "processed 1000000 by 03:27:53\n",
      "processed 1500000 by 03:30:12\n",
      "processed 2000000 by 03:32:49\n",
      "processed 2500000 by 03:35:08\n",
      "processed 3000000 by 03:37:29\n",
      "processed 3500000 by 03:39:47\n",
      "processed 4000000 by 03:42:10\n",
      "processed 4500000 by 03:44:35\n",
      "processed 5000000 by 03:46:53\n",
      "processed 5500000 by 03:49:11\n",
      "processed 6000000 by 03:51:38\n",
      "processed 6500000 by 03:54:05\n",
      "processed 7000000 by 03:56:28\n",
      "processed 7500000 by 03:58:46\n",
      "processed 8000000 by 04:01:03\n",
      "processed 8500000 by 04:03:24\n",
      "processed 9000000 by 04:05:41\n",
      "processed 9500000 by 04:08:01\n",
      "processed 10000000 by 04:10:20\n",
      "processed 10500000 by 04:12:49\n",
      "processed 11000000 by 04:15:14\n",
      "processed 11500000 by 04:17:36\n",
      "processed 12000000 by 04:20:07\n",
      "processed 12500000 by 04:22:33\n",
      "processed 13000000 by 04:24:43\n",
      "processed 13500000 by 04:26:36\n",
      "processed 14000000 by 04:28:32\n",
      "processed 14500000 by 04:30:26\n",
      "processed 15000000 by 04:32:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 15500000 by 04:34:19\n",
      "processed 16000000 by 04:36:12\n",
      "processed 16500000 by 04:38:03\n",
      "processed 17000000 by 04:39:54\n",
      "processed 17500000 by 04:41:49\n",
      "processed 18000000 by 04:43:40\n",
      "processed 18500000 by 04:45:33\n",
      "processed 19000000 by 04:47:25\n",
      "processed 19500000 by 04:49:18\n",
      "processed 20000000 by 04:51:12\n",
      "processed 20500000 by 04:53:10\n",
      "processed 21000000 by 04:55:03\n",
      "processed 21500000 by 04:56:57\n",
      "processed 22000000 by 04:58:51\n",
      "processed 22500000 by 05:00:47\n",
      "processed 23000000 by 05:02:41\n",
      "processed 23500000 by 05:04:36\n",
      "processed 24000000 by 05:06:29\n",
      "processed 24500000 by 05:08:22\n",
      "processed 25000000 by 05:10:14\n",
      "processed 25500000 by 05:12:08\n",
      "processed 26000000 by 05:14:03\n",
      "processed 26500000 by 05:15:55\n",
      "processed 27000000 by 05:17:51\n",
      "processed 27500000 by 05:19:42\n",
      "processed 28000000 by 05:21:37\n",
      "processed 28500000 by 05:23:31\n",
      "processed 29000000 by 05:25:25\n",
      "processed 29500000 by 05:27:17\n",
      "processed 30000000 by 05:29:09\n",
      "processed 30500000 by 05:31:01\n",
      "processed 31000000 by 05:32:58\n",
      "processed 31500000 by 05:34:54\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-04.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-05.zst *****\n",
      "processed 500000 by 05:37:17\n",
      "processed 1000000 by 05:39:13\n",
      "processed 1500000 by 05:41:06\n",
      "processed 2000000 by 05:43:02\n",
      "processed 2500000 by 05:44:58\n",
      "processed 3000000 by 05:46:51\n",
      "processed 3500000 by 05:48:43\n",
      "processed 4000000 by 05:50:38\n",
      "processed 4500000 by 05:52:34\n",
      "processed 5000000 by 05:54:26\n",
      "processed 5500000 by 05:56:21\n",
      "processed 6000000 by 05:58:13\n",
      "processed 6500000 by 06:00:06\n",
      "processed 7000000 by 06:02:01\n",
      "processed 7500000 by 06:03:53\n",
      "processed 8000000 by 06:05:46\n",
      "processed 8500000 by 06:07:38\n",
      "processed 9000000 by 06:09:33\n",
      "processed 9500000 by 06:11:22\n",
      "processed 10000000 by 06:13:17\n",
      "processed 10500000 by 06:15:09\n",
      "processed 11000000 by 06:17:03\n",
      "processed 11500000 by 06:18:53\n",
      "processed 12000000 by 06:20:45\n",
      "processed 12500000 by 06:22:41\n",
      "processed 13000000 by 06:24:32\n",
      "processed 13500000 by 06:26:24\n",
      "processed 14000000 by 06:28:14\n",
      "processed 14500000 by 06:30:07\n",
      "processed 15000000 by 06:32:02\n",
      "processed 15500000 by 06:33:55\n",
      "processed 16000000 by 06:35:46\n",
      "processed 16500000 by 06:37:35\n",
      "processed 17000000 by 06:39:27\n",
      "processed 17500000 by 06:41:17\n",
      "processed 18000000 by 06:43:09\n",
      "processed 18500000 by 06:44:59\n",
      "processed 19000000 by 06:46:45\n",
      "processed 19500000 by 06:48:34\n",
      "processed 20000000 by 06:50:22\n",
      "processed 20500000 by 06:52:16\n",
      "processed 21000000 by 06:54:03\n",
      "processed 21500000 by 06:55:49\n",
      "processed 22000000 by 06:57:37\n",
      "processed 22500000 by 06:59:26\n",
      "processed 23000000 by 07:01:16\n",
      "processed 23500000 by 07:03:10\n",
      "processed 24000000 by 07:04:58\n",
      "processed 24500000 by 07:06:49\n",
      "processed 25000000 by 07:08:39\n",
      "processed 25500000 by 07:10:26\n",
      "processed 26000000 by 07:12:22\n",
      "processed 26500000 by 07:14:13\n",
      "processed 27000000 by 07:16:05\n",
      "processed 27500000 by 07:17:57\n",
      "processed 28000000 by 07:19:48\n",
      "processed 28500000 by 07:21:41\n",
      "processed 29000000 by 07:23:33\n",
      "processed 29500000 by 07:25:27\n",
      "processed 30000000 by 07:27:17\n",
      "processed 30500000 by 07:29:15\n",
      "processed 31000000 by 07:31:10\n",
      "processed 31500000 by 07:33:05\n",
      "processed 32000000 by 07:34:57\n",
      "processed 32500000 by 07:36:52\n",
      "processed 33000000 by 07:38:44\n",
      "processed 33500000 by 07:40:35\n",
      "processed 34000000 by 07:42:29\n",
      "processed 34500000 by 07:44:22\n",
      "processed 35000000 by 07:46:19\n",
      "processed 35500000 by 07:48:14\n",
      "processed 36000000 by 07:50:08\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-05.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "***** Start processing for RS_2021-06.zst *****\n",
      "processed 500000 by 07:53:15\n",
      "processed 1000000 by 07:55:11\n",
      "processed 1500000 by 07:57:05\n",
      "processed 2000000 by 07:59:01\n",
      "processed 2500000 by 08:00:55\n",
      "processed 3000000 by 08:02:49\n",
      "processed 3500000 by 08:04:58\n",
      "processed 4000000 by 08:06:54\n",
      "processed 4500000 by 08:08:49\n",
      "processed 5000000 by 08:10:43\n",
      "processed 5500000 by 08:12:36\n",
      "processed 6000000 by 08:14:31\n",
      "processed 6500000 by 08:16:23\n",
      "processed 7000000 by 08:18:19\n",
      "processed 7500000 by 08:20:17\n",
      "processed 8000000 by 08:22:15\n",
      "processed 8500000 by 08:24:07\n",
      "processed 9000000 by 08:26:02\n",
      "processed 9500000 by 08:28:01\n",
      "processed 10000000 by 08:29:57\n",
      "processed 10500000 by 08:31:52\n",
      "processed 11000000 by 08:33:44\n",
      "processed 11500000 by 08:35:37\n",
      "processed 12000000 by 08:37:36\n",
      "processed 12500000 by 08:39:30\n",
      "processed 13000000 by 08:41:27\n",
      "processed 13500000 by 08:43:34\n",
      "processed 14000000 by 08:45:35\n",
      "processed 14500000 by 08:47:42\n",
      "processed 15000000 by 08:49:37\n",
      "processed 15500000 by 08:51:36\n",
      "processed 16000000 by 08:53:41\n",
      "processed 16500000 by 08:55:43\n",
      "processed 17000000 by 08:57:40\n",
      "processed 17500000 by 08:59:39\n",
      "processed 18000000 by 09:01:34\n",
      "processed 18500000 by 09:03:39\n",
      "processed 19000000 by 09:05:35\n",
      "processed 19500000 by 09:07:32\n",
      "processed 20000000 by 09:09:24\n",
      "processed 20500000 by 09:11:22\n",
      "processed 21000000 by 09:13:21\n",
      "processed 21500000 by 09:15:15\n",
      "processed 22000000 by 09:17:12\n",
      "processed 22500000 by 09:19:16\n",
      "processed 23000000 by 09:21:10\n",
      "processed 23500000 by 09:23:17\n",
      "processed 24000000 by 09:25:14\n",
      "processed 24500000 by 09:27:16\n",
      "processed 25000000 by 09:29:07\n",
      "processed 25500000 by 09:31:08\n",
      "processed 26000000 by 09:33:15\n",
      "processed 26500000 by 09:35:22\n",
      "processed 27000000 by 09:37:22\n",
      "processed 27500000 by 09:39:27\n",
      "processed 28000000 by 09:41:22\n",
      "processed 28500000 by 09:43:20\n",
      "processed 29000000 by 09:45:14\n",
      "processed 29500000 by 09:47:12\n",
      "processed 30000000 by 09:49:12\n",
      "processed 30500000 by 09:51:20\n",
      "processed 31000000 by 09:53:24\n",
      "processed 31500000 by 09:55:33\n",
      "processed 32000000 by 09:57:38\n",
      "processed 32500000 by 09:59:38\n",
      "processed 33000000 by 10:01:37\n",
      "processed 33500000 by 10:03:47\n",
      "processed 34000000 by 10:05:45\n",
      "-------------------------------- Done reading, will write files now --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "-------------------------------- Done processing for RS_2021-06.zst --------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "finish time: 2022-03-17 10:06:19.517506\n"
     ]
    }
   ],
   "source": [
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "counter = 0\n",
    "for zst_file in zst_files:\n",
    "    ns_subreddit = defaultdict(dict)\n",
    "    # counting how many time a news source appears in each subreddit along with weighted counts\n",
    "    print(\"***** Start processing for {} *****\".format(zst_file))\n",
    "    with open(\"E://thesis_data/\"+zst_file, 'rb') as ifh: #, open(\"stream_output.json\", 'wb') as ofh:\n",
    "        with dctx.stream_reader(ifh, read_size=2) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            for d in text_stream:\n",
    "                line = json.loads(d)\n",
    "                subreddit, subreddit_id = line['subreddit'], line['subreddit_id']\n",
    "                num_comments = line['num_comments']\n",
    "                upvote_ratio = line['upvote_ratio']\n",
    "                if subreddit not in subreddit_srid:\n",
    "                    subreddit_srid[subreddit] = subreddit_id\n",
    "                URLs = findURLs(line['url']) + findURLs(line['selftext'])                \n",
    "                hostnames = [get_hostname(url) for url in URLs]\n",
    "                # the following counter to count how many times each url appears in the post\n",
    "                URLs = Counter([element for sub in hostnames for element in sub])\n",
    "                for url in URLs:\n",
    "                    if url in news_sources:\n",
    "                        mention_count = URLs[url]\n",
    "                        weighted_by_upvote_ratio = URLs[url]*upvote_ratio\n",
    "                        weighted_by_num_comments = URLs[url]*num_comments\n",
    "                        # update\n",
    "                        triple = (mention_count, weighted_by_upvote_ratio, weighted_by_num_comments)\n",
    "                        # if subreddit in ns_subreddit[url], update. Else, initialize\n",
    "                        if subreddit not in ns_subreddit[url]:\n",
    "                            ns_subreddit[url][subreddit] = triple\n",
    "                        else:\n",
    "                            ns_subreddit[url][subreddit] = (ns_subreddit[url][subreddit][0] + triple[0],\n",
    "                                                            ns_subreddit[url][subreddit][1] + triple[1],\n",
    "                                                            ns_subreddit[url][subreddit][2] + triple[2])\n",
    "                counter += 1\n",
    "                if counter%500000 == 0: \n",
    "                    print(\"processed {} by {}\".format(counter, str(datetime.datetime.now())[11:19]))\n",
    "                \n",
    "    \n",
    "    print(\"-------------------------------- Done reading, will write files now --------------------------------\")\n",
    "    \n",
    "    # write into files separated by months\n",
    "    with open(\"ns_subreddit_{}.json\".format(zst_file[3:10]), \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(ns_subreddit, outfile, indent=4)\n",
    "        \n",
    "    # with open(\"subreddit_ns_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as outfile1:\n",
    "    #     json.dump(subreddit_ns, outfile1, indent=4)\n",
    "        \n",
    "    counter = 0\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------- Done processing for {} --------------------------------\".format(zst_file))\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "                \n",
    "with open (\"subreddit_srid_{}.json\".format(zst_file[3:10]), \"w\", encoding = \"utf-8\") as infile_srid:\n",
    "    json.dump(subreddit_srid, infile_srid, indent=4)\n",
    "\n",
    "print(\"finish time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

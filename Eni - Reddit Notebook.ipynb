{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the Reddit datasets are compressed as a ZST archive. I found the following code in the materials about PushShift.\n",
    "\n",
    "The following shows the extraction of links for only one month. If one downloads the files for 12 months, this can be run a couple of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Simple Class to Read the Zipped Archive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        self.fh = open(file,'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode()\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instatiate the reader of the archive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specified\n",
    "reader = Zreader(\"RC_2019-06.zst\", chunk_size=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store only partial posts that contain a URL in the body**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6041437\n"
     ]
    }
   ],
   "source": [
    "postCnt = 0\n",
    "postsWithURLs = []\n",
    "\n",
    "for line in reader.readlines():\n",
    "    obj = json.loads(line)\n",
    "    postCnt += 1\n",
    "    if ('body' in obj) and ('http' in obj['body']):\n",
    "        postsWithURLs.append({key: obj[key] for key in ['body', 'created_utc', \n",
    "                                               'score', 'subreddit'] if key in obj})\n",
    "\n",
    "        \n",
    "print(len(postsWithURLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of posts with URLs as a portion of all posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04502723906196912"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(postsWithURLs)/postCnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 4.5% of all posts contain at list one link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of a partial post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': \"Why haven't you followed[ this twitter account](https://twitter.com/UNmigration) yet?\",\n",
       " 'created_utc': 1559407265,\n",
       " 'score': 2,\n",
       " 'subreddit': 'neoliberal'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postsWithURLs[108010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract URLs from text\n",
    "\n",
    "I found the following function on the Web. I'm not convinced that it's the best way to retrieve URLs. The URLs are not very clean. For the moment it might be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def findURLs(string):\n",
    "    \"\"\"One possible way to extract URLs from text.\n",
    "    It finds all URLs in a long string.\n",
    "    \"\"\"\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)      \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** The function above is too slow, it is not be used. Below is a faster version to extract URLs from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findU(phrase):\n",
    "    regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)')\n",
    "    url = re.findall(regex, phrase)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add the URLs to the existing dataset of `postsWithURLs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in postsWithURLs:\n",
    "    post['urls'] = findU(post['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': '[Just gave $27.](https://imgur.com/FhFdthj)',\n",
       " 'created_utc': 1559347203,\n",
       " 'score': 2,\n",
       " 'subreddit': 'SandersForPresident',\n",
       " 'urls': ['https://imgur.com/FhFdthj']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postsWithURLs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6041437"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done = [p for p in postsWithURLs if len(p) == 5]\n",
    "len(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find total number of unique URLs / unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueURLs = Counter()\n",
    "\n",
    "for post in postsWithURLs:\n",
    "    urls = post['urls']\n",
    "    for url in urls:\n",
    "        uniqueURLs[url] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5373689"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueURLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.reddit.com/message/compose?to=/r/dankmemes)', 209598),\n",
       " ('https://www.reddit.com/r/TranscribersOfReddit/wiki/index)', 200912),\n",
       " ('https://github.com/GrafeasGroup/tor)', 142651),\n",
       " ('https://www.reddit.com/message/compose?to=%2Fr%2FTranscribersOfReddit&amp;subject=Bot%20Question&amp;message=)',\n",
       "  142651),\n",
       " ('https://www.reddit.com/r/PewdiepieSubmissions/comments/c0m06h/introducing_community_moderation/)',\n",
       "  63539),\n",
       " ('https://discord.gg/BzUnwjt)', 60620),\n",
       " ('https://www.reddit.com/r/Market76/comments/biz92l/ign_megathread/)', 60618),\n",
       " ('https://reddit.com/message/compose?to=%2Fr%2FMarket76)', 60566),\n",
       " ('https://www.reddit.com/r/Market76/comments/bj7uzm/43019_big_subreddit_changes/)',\n",
       "  60562),\n",
       " ('https://www.reddit.com/r/Market76/wiki/index)', 60562)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueURLs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the domain name and organize links by domain, together with their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "linksCounter = defaultdict(Counter)\n",
    "\n",
    "for url in uniqueURLs:\n",
    "    domain = urlparse(url).netloc\n",
    "    linksCounter[domain][url] = uniqueURLs[url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367769"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(linksCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a counter for domains and their total links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainsWithCounts = Counter()\n",
    "\n",
    "for dom in linksCounter:\n",
    "    domainsWithCounts[dom] = sum(linksCounter[dom].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('www.reddit.com', 4803689),\n",
       " ('www.youtube.com', 482683),\n",
       " ('np.reddit.com', 401595),\n",
       " ('github.com', 381511),\n",
       " ('imgur.com', 367810),\n",
       " ('youtu.be', 361313),\n",
       " ('i.imgur.com', 324031),\n",
       " ('reddit.com', 222838),\n",
       " ('en.wikipedia.org', 209967),\n",
       " ('discord.gg', 180598)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainsWithCounts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many links are simply reddit?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = [dom for dom in domainsWithCounts if 'reddit' in dom]\n",
    "len(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.reddit.com',\n",
       " 'www\\\\.reddit\\\\.com',\n",
       " 'reddit.com',\n",
       " 'old.reddit.com',\n",
       " 'np.reddit.com',\n",
       " 'contact.dankmemesreddit.com)',\n",
       " 'reddit.comepnk3a7)',\n",
       " 'reddit.zendesk.com',\n",
       " 'redditsearch.io',\n",
       " 'redditmetrics.com']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5610232"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumReddit = sum([domainsWithCounts[dom] for dom in reddit])\n",
    "sumReddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42330070657359564"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumReddit/sum(uniqueURLs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** 42% of all links belong to Reddit domain itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(domainsWithCounts, open('june2020-domains.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
